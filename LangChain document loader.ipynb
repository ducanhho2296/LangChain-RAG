{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents Using LangChain for Different Sources \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders.csv_loader import UnstructuredCSVLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from TXT files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-06 20:20:31--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6363 (6.2K) [text/plain]\n",
      "Saving to: ‘new-Policies.txt’\n",
      "\n",
      "new-Policies.txt    100%[===================>]   6.21K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-01-06 20:20:31 (1.61 GB/s) - ‘new-Policies.txt’ saved [6363/6363]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x7ff495ba6790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"new-Policies.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content=\"1. Code of Conduct\\n\\nOur Code of Conduct establishes the core values and ethical standards that all members of our organization must adhere to. We are committed to fostering a workplace characterized by integrity, respect, and accountability.\\n\\nIntegrity: We commit to the highest ethical standards by being honest and transparent in all our dealings, whether with colleagues, clients, or the community. We protect sensitive information and avoid conflicts of interest.\\n\\nRespect: We value diversity and every individual's contribution. Discrimination, harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where differences are respected, and everyone is treated with dignity.\\n\\nAccountability: We are responsible for our actions and decisions, complying with all relevant laws and regulations. We aim for continuous improvement and report any breaches of this code, supporting investigations into such matters.\\n\\nSafety: We prioritize the safety of our employees, clients, and the community. We encourage a culture of safety, including reporting any unsafe practices or conditions.\\n\\nEnvironmental Responsibility: We strive to reduce our environmental impact and promote sustainable practices.\\n\\nThis Code of Conduct is the cornerstone of our organizational culture. We expect every employee to uphold these principles and act as role models, ensuring our reputation for ethical conduct, integrity, and social responsibility.\\n\\n2. Recruitment Policy\\n\\nOur Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified and diverse candidates into our organization. The success of our company depends on the talent, skills, and commitment of our employees.\\n\\nEqual Opportunity: We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively support diversity and inclusion.\\n\\nTransparency: We maintain a transparent recruitment process. Job vacancies are advertised both internally and externally when appropriate. Job descriptions and requirements are clear and accurately reflect the role.\\n\\nSelection Criteria: We base our selection on qualifications, experience, and skills relevant to the role. Our interviews and assessments are objective, and decisions are made impartially.\\n\\nData Privacy: We are dedicated to protecting candidates' personal information and comply with all applicable data protection laws.\\n\\nFeedback: Candidates receive timely and constructive feedback on their applications and interview performance.\\n\\nOnboarding: New hires receive thorough onboarding to help them integrate effectively, including an overview of our culture, policies, and expectations.\\n\\nEmployee Referrals: We welcome employee referrals as they help build a strong and engaged team.\\n\\nThis policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\\n\\n\\n3. Internet and Email Policy\\n\\nOur Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\\n\\nAcceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\\n\\nSecurity: Protect your login credentials and avoid sharing passwords. Be cautious with email attachments and links from unknown sources, and promptly report any unusual online activity or potential security threats.\\n\\nConfidentiality: Use email for confidential information, trade secrets, and sensitive customer data only with encryption. Be careful when discussing company matters on public platforms or social media.\\n\\nHarassment and Inappropriate Content: Internet and email must not be used for harassment, discrimination, or the distribution of offensive content. Always communicate respectfully and sensitively online.\\n\\nCompliance: Adhere to all relevant laws and regulations concerning internet and email use, including copyright and data protection laws.\\n\\nMonitoring: The company reserves the right to monitor internet and email usage for security and compliance purposes.\\n\\nConsequences: Violations of this policy may lead to disciplinary action, including potential termination.\\n\\nThis policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\\n\\n4. Mobile Phone Policy\\n\\nOur Mobile Phone Policy defines standards for responsible use of mobile devices within the organization to ensure alignment with company values and legal requirements.\\n\\nAcceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is allowed if it does not disrupt work responsibilities.\\n\\nSecurity: Secure your mobile device and credentials. Be cautious with app downloads and links from unknown sources, and report any security issues promptly.\\n\\nConfidentiality: Avoid sharing sensitive company information via unsecured messaging apps or emails. Exercise caution when discussing company matters in public.\\n\\nCost Management: Personal use of mobile phones should be separate from company accounts, and any personal charges on company-issued phones must be reimbursed.\\n\\nCompliance: Comply with all relevant laws and regulations concerning mobile phone usage, including data protection and privacy laws.\\n\\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\\n\\nConsequences: Non-compliance with this policy may result in disciplinary actions, including potential loss of mobile phone privileges.\\n\\nThis policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.\\n\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1. Code of Conduct\\n'\n",
      " '\\n'\n",
      " 'Our Code of Conduct establishes the core values and ethical standards that '\n",
      " 'all members of our organization must adhere to. We are committed to '\n",
      " 'fostering a workplace characterized by integrity, respect, and '\n",
      " 'accountability.\\n'\n",
      " '\\n'\n",
      " 'Integrity: We commit to the highest ethical standards by being honest and '\n",
      " 'transparent in all our dealings, whether with colleagues, clients, or the '\n",
      " 'community. We protect sensitive information and avoid conflicts of '\n",
      " 'interest.\\n'\n",
      " '\\n'\n",
      " \"Respect: We value diversity and every individual's contribution. \"\n",
      " 'Discrimination, harassment, or any form of disrespect is not tolerated. We '\n",
      " 'promote an inclusive environment where differences are respected, and '\n",
      " 'everyone is treated with dignity.\\n'\n",
      " '\\n'\n",
      " 'Accountability: We are responsible for our actions and decisions, complying '\n",
      " 'with all relevant laws and regulations. We aim for continuous improvement '\n",
      " 'and report any breaches of this code, supporting investigations into such '\n",
      " 'matters.\\n'\n",
      " '\\n'\n",
      " 'Safety: We prioritize the safety of our employees, c')\n"
     ]
    }
   ],
   "source": [
    "pprint(data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from PDF files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_url)\n",
    "\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first page of the PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LAB: L ARGE -SCALE ALIGNMENT FOR CHATBOTS\n",
      "MIT-IBM Watson AI Lab and IBM Research\n",
      "Shivchander Sudalairaj∗\n",
      "Abhishek Bhandwaldar∗\n",
      "Aldo Pareja∗\n",
      "Kai Xu\n",
      "David D. Cox\n",
      "Akash Srivastava∗,†\n",
      "*Equal Contribution, †Corresponding Author\n",
      "ABSTRACT\n",
      "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
      "ology designed to overcome the scalability challenges in the instruction-tuning\n",
      "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
      "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
      "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
      "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
      "itive performance across several benchmarks compared to models trained with\n",
      "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
      "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
      "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
      "step forward in the efficient training of LLMs for a wide range of applications.\n",
      "1 I NTRODUCTION\n",
      "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
      "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
      "marization . This has been made possible, in large part, by the introduction of the transformer\n",
      "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
      "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
      "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
      "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
      "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
      "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
      "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
      "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
      "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
      "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
      "article in 2 lines: {News article }) and the model is trained to maximize the likelihood of the pro-\n",
      "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
      "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
      "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
      "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
      "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
      "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
      "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
      "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
      "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
      "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
      "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
      "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
      "1arXiv:2403.01081v3  [cs.CL]  29 Apr 2024' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first three pages of the PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page number 1\n",
      "page_content='LAB: L ARGE -SCALE ALIGNMENT FOR CHATBOTS\n",
      "MIT-IBM Watson AI Lab and IBM Research\n",
      "Shivchander Sudalairaj∗\n",
      "Abhishek Bhandwaldar∗\n",
      "Aldo Pareja∗\n",
      "Kai Xu\n",
      "David D. Cox\n",
      "Akash Srivastava∗,†\n",
      "*Equal Contribution, †Corresponding Author\n",
      "ABSTRACT\n",
      "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
      "ology designed to overcome the scalability challenges in the instruction-tuning\n",
      "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
      "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
      "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
      "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
      "itive performance across several benchmarks compared to models trained with\n",
      "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
      "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
      "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
      "step forward in the efficient training of LLMs for a wide range of applications.\n",
      "1 I NTRODUCTION\n",
      "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
      "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
      "marization . This has been made possible, in large part, by the introduction of the transformer\n",
      "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
      "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
      "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
      "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
      "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
      "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
      "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
      "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
      "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
      "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
      "article in 2 lines: {News article }) and the model is trained to maximize the likelihood of the pro-\n",
      "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
      "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
      "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
      "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
      "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
      "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
      "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
      "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
      "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
      "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
      "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
      "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
      "1arXiv:2403.01081v3  [cs.CL]  29 Apr 2024' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0}\n",
      "page number 2\n",
      "page_content='training captures enough of the distribution of language and knowledge, such that a small amount of\n",
      "supervised training can “unlock” or shape latent abilities related to the ultimate desired instruction-\n",
      "following behavior of the model. However, unlike the unstructured data that is abundantly available\n",
      "in the public domain, high-quality, human-generated task-specific instruction data is costly to pro-\n",
      "cure, even via crowd-sourcing, and human-generated instruction data is typically closely guarded\n",
      "by model builders, even for ostensibly “open” model-building efforts. In this work, we address\n",
      "the challenges associated with scaling of the alignment-tuning phase and propose a new method\n",
      "called LAB: Large-scale Alignment for chatBots. The LAB method consists of two components:\n",
      "(i) a taxonomy-guided synthetic data generation method and quality assurance process that yields a\n",
      "highly diverse and high-quality instruction dataset, without resorting to the use of proprietary LLMs\n",
      "like GPT-4 or substantial human curation, and (ii) a novel multi-phase training framework and un-\n",
      "conventional tuning regime that allows for adding new knowledge and instruction-following abilities\n",
      "into pre-trained LLMs without suffering from catastrophic forgetting. Our findings show that LAB-\n",
      "trained models can perform competitively with proprietary and open-source models that use human\n",
      "annotations and/or synthetic data generated using GPT-4 on a number of benchmarks.\n",
      "2 R ELATED WORK\n",
      "Existing methods for instruction tuning typically either rely on humans for generating high-quality\n",
      "datasets, or use synthetic data generation using a large teacher model. OpenAI (Ouyang et al.,\n",
      "2022) arguably set the standard for model alignment from human data, employing human annota-\n",
      "tors to gather data for supervised fine tuning (SFT) and reinforcement learning with human feed-\n",
      "back (RLHF) training. Collecting human-generated data for these steps is complex undertaking; the\n",
      "selection of annotators requires a rigorous multi-stage screening process aimed at achieving high\n",
      "inter-annotator agreement, and collecting even modest amounts data (by LLM standards) requires\n",
      "the coordination of large groups of annotators. The creators of the LLaMA 2 model series (Touvron\n",
      "et al., 2023) followed a similar recipe, collecting tens of thousands of human-generated instruction\n",
      "samples, and approximately 1 million human-annotated binary comparisons for reward modeling.\n",
      "Not only are such approaches expensive and time consuming, but they can also potentially limit\n",
      "agility in exploring the space of instructions and capabilities the model is trained to perform. Alter-\n",
      "natives to this approach, such as transforming existing human datasets into instructions via templat-\n",
      "ing (Wei et al.) can be more cost effective, but face limitations in the naturalness and length of the\n",
      "responses used for training.\n",
      "More recently, training with synthetic data generated from LLMs has emerged as an alternative to\n",
      "purely human-data-based approaches. Wang et al. (2023) introduced Self-Instruct, which leverages\n",
      "a small number of handwritten human seed instructions as input to bootstrapping process to generate\n",
      "a large number of samples using an LLM’s own generation abilities. Taori et al. (2023) built upon\n",
      "Self-Instruct, using a larger teacher model to generate synthetic data to train a smaller student model,\n",
      "and incorporating principles in the generation prompt to promote diversity in the generated instruc-\n",
      "tion data. Xu et al. (2023) introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes\n",
      "iteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee\n",
      "et al. (2023), Mitra et al. (2023) present a synthetic data generation approach to enhance task di-\n",
      "versity and scalability, alongside a progressive training framework aimed at improving the model’s\n",
      "reasoning ability and response style to match teacher models. This is achieved by generating rich' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 1}\n",
      "page number 3\n",
      "page_content='versity and scalability, alongside a progressive training framework aimed at improving the model’s\n",
      "reasoning ability and response style to match teacher models. This is achieved by generating rich\n",
      "reasoning signals in the generated answer and progressively training on datasets of varying difficulty\n",
      "in incremental phases.\n",
      "Similar to LAB, concurrent work, GLAN (Li et al., 2024), employs a semi-automatic approach to\n",
      "synthetic data generation that uses a human-curated taxonomy to generate instruction tuning data\n",
      "from a teacher model. However, as explained in section 3.2.2, unlike LAB, GLAN cannot be used\n",
      "to generate synthetic data from domains that are not captured in the teacher model’s support. As\n",
      "such, while LAB uses the open-source Mixtral model as the teacher, like many other synthetic\n",
      "data generation approaches, GLAN has to rely on a large proprietary model (GPT-4). This poses\n",
      "complicated questions about the usability of generated data (especially for commercial purposes)\n",
      "since the terms of use of proprietary models typically forbid using the model to improve other\n",
      "models.\n",
      "2' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "for p,page in enumerate(pages[0:3]):\n",
    "    print(f\"page number {p+1}\")\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyMuPDFLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyMuPDFLoader` is the fastest of the PDF parsing options. It provides detailed metadata about the PDF and its pages, and returns one document per page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyMuPDFLoader at 0x7ff4ad5aacd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyMuPDFLoader(pdf_url)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LAB: LARGE-SCALE ALIGNMENT FOR CHATBOTS\n",
      "MIT-IBM Watson AI Lab and IBM Research\n",
      "Shivchander Sudalairaj∗\n",
      "Abhishek Bhandwaldar∗\n",
      "Aldo Pareja∗\n",
      "Kai Xu\n",
      "David D. Cox\n",
      "Akash Srivastava∗,†\n",
      "*Equal Contribution, †Corresponding Author\n",
      "ABSTRACT\n",
      "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
      "ology designed to overcome the scalability challenges in the instruction-tuning\n",
      "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
      "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
      "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
      "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
      "itive performance across several benchmarks compared to models trained with\n",
      "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
      "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
      "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
      "step forward in the efficient training of LLMs for a wide range of applications.\n",
      "1\n",
      "INTRODUCTION\n",
      "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
      "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
      "marization . This has been made possible, in large part, by the introduction of the transformer\n",
      "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
      "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
      "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
      "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
      "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
      "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
      "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
      "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
      "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
      "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
      "article in 2 lines: {News article}) and the model is trained to maximize the likelihood of the pro-\n",
      "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
      "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
      "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
      "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
      "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
      "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
      "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
      "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
      "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
      "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
      "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
      "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
      "1\n",
      "arXiv:2403.01081v3  [cs.CL]  29 Apr 2024\n",
      "' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0, 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240501000524Z', 'modDate': 'D:20240501000524Z', 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `metadata` attribute reveals that `PyMuPDFLoader` provides more detailed metadata information than `PyPDFLoader`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Markdown files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-06 20:20:32--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eMSP5vJjj9yOfAacLZRWsg/markdown-sample.md\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3398 (3.3K) [text/markdown]\n",
      "Saving to: ‘markdown-sample.md’\n",
      "\n",
      "markdown-sample.md  100%[===================>]   3.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-01-06 20:20:32 (1.13 GB/s) - ‘markdown-sample.md’ saved [3398/3398]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eMSP5vJjj9yOfAacLZRWsg/markdown-sample.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader at 0x7ff4ac6e24d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_path = \"markdown-sample.md\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'markdown-sample.md'}, page_content='An h1 header\\n\\nParagraphs are separated by a blank line.\\n\\n2nd paragraph. Italic, bold, and monospace. Itemized lists\\nlook like:\\n\\nthis one\\n\\nthat one\\n\\nthe other one\\n\\nNote that --- not considering the asterisk --- the actual text\\ncontent starts at 4-columns in.\\n\\nBlock quotes are\\nwritten like so.\\n\\nThey can span multiple paragraphs,\\nif you like.\\n\\nUse 3 dashes for an em-dash. Use 2 dashes for ranges (ex., \"it\\'s all\\nin chapters 12--14\"). Three dots ... will be converted to an ellipsis.\\nUnicode is supported. ☺\\n\\nAn h2 header\\n\\nHere\\'s a numbered list:\\n\\nfirst item\\n\\nsecond item\\n\\nthird item\\n\\nNote again how the actual text starts at 4 columns in (4 characters\\nfrom the left side). Here\\'s a code sample:\\n\\nAs you probably guessed, indented 4 spaces. By the way, instead of\\nindenting the block, you can use delimited blocks, if you like:\\n\\n~~~\\ndefine foobar() {\\n    print \"Welcome to flavor country!\";\\n}\\n~~~\\n\\n(which makes copying & pasting easier). You can optionally mark the\\ndelimited block for Pandoc to syntax highlight it:\\n\\n~~~python\\nimport time\\n\\nQuick, count to ten!\\n\\nfor i in range(10):\\n    # (but not too quick)\\n    time.sleep(0.5)\\n    print i\\n~~~\\n\\nAn h3 header\\n\\nNow a nested list:\\n\\nFirst, get these ingredients:\\n\\ncarrots\\ncelery\\nlentils\\n\\nBoil some water.\\n\\nDump everything in the pot and follow\\n    this algorithm:\\nfind wooden spoon\\nuncover pot\\nstir\\ncover pot\\nbalance wooden spoon precariously on pot handle\\nwait 10 minutes\\ngoto first step (or shut off burner when done)\\n\\nDo not bump wooden spoon or it will fall.\\n\\nNotice again how text always lines up on 4-space indents (including\\nthat last line which continues item 3 above).\\n\\nHere\\'s a link to a website, to a local\\ndoc, and to a section heading in the current\\ndoc. Here\\'s a footnote [^1].\\n\\n[^1]: Footnote text goes here.\\n\\nTables can look like this:\\n\\nsize  material      color\\n\\n9     leather       brown\\n10    hemp canvas   natural\\n11    glass         transparent\\n\\nTable: Shoes, their sizes, and what they\\'re made of\\n\\n(The above is the caption for the table.) Pandoc also supports\\nmulti-line tables:\\n\\nkeyword   text\\n\\nred       Sunsets, apples, and\\n          other red or reddish\\n          things.\\n\\ngreen     Leaves, grass, frogs\\n          and other things it\\'s\\n          not easy being.\\n\\nA horizontal rule follows.\\n\\nHere\\'s a definition list:\\n\\napples\\n  : Good for making applesauce.\\noranges\\n  : Citrus!\\ntomatoes\\n  : There\\'s no \"e\" in tomatoe.\\n\\nAgain, text is indented 4 spaces. (Put a blank line between each\\nterm/definition pair to spread things out more.)\\n\\nHere\\'s a \"line block\":\\n\\n| Line one\\n|   Line too\\n| Line tree\\n\\nand images can be specified like so:\\n\\nInline math equations go in like so: $\\\\omega = d\\\\phi / dt$. Display\\nmath should get its own line and be put in in double-dollarsigns:\\n\\n$$I = \\\\int \\\\rho R^{2} dV$$\\n\\nAnd note that you can backslash-escape any punctuation characters\\nwhich you wish to be displayed literally, ex.: `foo`, *bar*, etc.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from JSON files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-06 20:20:35--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hAmzVJeOUAMHzmhUHNdAUg/facebook-chat.json\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2167 (2.1K) [application/json]\n",
      "Saving to: ‘facebook-chat.json’\n",
      "\n",
      "facebook-chat.json  100%[===================>]   2.12K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-01-06 20:20:35 (324 MB/s) - ‘facebook-chat.json’ saved [2167/2167]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hAmzVJeOUAMHzmhUHNdAUg/facebook-chat.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='facebook-chat.json'\n",
    "data = json.loads(Path(file_path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'},\n",
      " 'is_still_participant': True,\n",
      " 'joinable_mode': {'link': '', 'mode': 1},\n",
      " 'magic_words': [],\n",
      " 'messages': [{'content': 'Bye!',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675597571851},\n",
      "              {'content': 'Oh no worries! Bye',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675597435669},\n",
      "              {'content': 'No Im sorry it was my mistake, the blue one is not '\n",
      "                          'for sale',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675596277579},\n",
      "              {'content': 'I thought you were selling the blue one!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675595140251},\n",
      "              {'content': 'Im not interested in this bag. Im interested in the '\n",
      "                          'blue one!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675595109305},\n",
      "              {'content': 'Here is $129',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595068468},\n",
      "              {'photos': [{'creation_timestamp': 1675595059,\n",
      "                           'uri': 'url_of_some_picture.jpg'}],\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595060730},\n",
      "              {'content': 'Online is at least $100',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595045152},\n",
      "              {'content': 'How much do you want?',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675594799696},\n",
      "              {'content': 'Goodmorning! $50 is too low.',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675577876645},\n",
      "              {'content': 'Hi! Im interested in your bag. Im offering $50. Let '\n",
      "                          'me know if you are interested. Thanks!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675549022673}],\n",
      " 'participants': [{'name': 'User 1'}, {'name': 'User 2'}],\n",
      " 'thread_path': 'inbox/User 1 and User 2 chat',\n",
      " 'title': 'User 1 and User 2 chat'}\n"
     ]
    }
   ],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `JSONLoader` to load data from the JSON file. However, JSON files can have various attribute-value pairs. If we want to load a specific attribute and its value, we need to set an appropriate `jq schema`.\n",
    "\n",
    "So for example, if we want to load the `content` from the JSON file, we need to set `jq_schema='.messages[].content'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path=file_path,\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 1}, page_content='Bye!'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 2}, page_content='Oh no worries! Bye'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 3}, page_content='No Im sorry it was my mistake, the blue one is not for sale'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 4}, page_content='I thought you were selling the blue one!'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 5}, page_content='Im not interested in this bag. Im interested in the blue one!'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 6}, page_content='Here is $129'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 7}, page_content=''),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 8}, page_content='Online is at least $100'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 9}, page_content='How much do you want?'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 10}, page_content='Goodmorning! $50 is too low.'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 11}, page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!')]\n"
     ]
    }
   ],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from CSV files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-06 20:38:15--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IygVG_j0M87BM4Z0zFsBMA/mlb-teams-2012.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 848 [text/csv]\n",
      "Saving to: ‘mlb-teams-2012.csv’\n",
      "\n",
      "mlb-teams-2012.csv  100%[===================>]     848  --.-KB/s    in 0s      \n",
      "\n",
      "2025-01-06 20:38:16 (205 MB/s) - ‘mlb-teams-2012.csv’ saved [848/848]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IygVG_j0M87BM4Z0zFsBMA/mlb-teams-2012.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='mlb-teams-2012.csv')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 0}, page_content='Team: Nationals\\n\"Payroll (millions)\": 81.34\\n\"Wins\": 98'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 1}, page_content='Team: Reds\\n\"Payroll (millions)\": 82.20\\n\"Wins\": 97'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 2}, page_content='Team: Yankees\\n\"Payroll (millions)\": 197.96\\n\"Wins\": 95'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 3}, page_content='Team: Giants\\n\"Payroll (millions)\": 117.62\\n\"Wins\": 94'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 4}, page_content='Team: Braves\\n\"Payroll (millions)\": 83.31\\n\"Wins\": 94'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 5}, page_content='Team: Athletics\\n\"Payroll (millions)\": 55.37\\n\"Wins\": 94'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 6}, page_content='Team: Rangers\\n\"Payroll (millions)\": 120.51\\n\"Wins\": 93'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 7}, page_content='Team: Orioles\\n\"Payroll (millions)\": 81.43\\n\"Wins\": 93'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 8}, page_content='Team: Rays\\n\"Payroll (millions)\": 64.17\\n\"Wins\": 90'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 9}, page_content='Team: Angels\\n\"Payroll (millions)\": 154.49\\n\"Wins\": 89'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 10}, page_content='Team: Tigers\\n\"Payroll (millions)\": 132.30\\n\"Wins\": 88'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 11}, page_content='Team: Cardinals\\n\"Payroll (millions)\": 110.30\\n\"Wins\": 88'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 12}, page_content='Team: Dodgers\\n\"Payroll (millions)\": 95.14\\n\"Wins\": 86'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 13}, page_content='Team: White Sox\\n\"Payroll (millions)\": 96.92\\n\"Wins\": 85'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 14}, page_content='Team: Brewers\\n\"Payroll (millions)\": 97.65\\n\"Wins\": 83'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 15}, page_content='Team: Phillies\\n\"Payroll (millions)\": 174.54\\n\"Wins\": 81'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 16}, page_content='Team: Diamondbacks\\n\"Payroll (millions)\": 74.28\\n\"Wins\": 81'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 17}, page_content='Team: Pirates\\n\"Payroll (millions)\": 63.43\\n\"Wins\": 79'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 18}, page_content='Team: Padres\\n\"Payroll (millions)\": 55.24\\n\"Wins\": 76'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 19}, page_content='Team: Mariners\\n\"Payroll (millions)\": 81.97\\n\"Wins\": 75'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 20}, page_content='Team: Mets\\n\"Payroll (millions)\": 93.35\\n\"Wins\": 74'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 21}, page_content='Team: Blue Jays\\n\"Payroll (millions)\": 75.48\\n\"Wins\": 73'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 22}, page_content='Team: Royals\\n\"Payroll (millions)\": 60.91\\n\"Wins\": 72'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 23}, page_content='Team: Marlins\\n\"Payroll (millions)\": 118.07\\n\"Wins\": 69'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 24}, page_content='Team: Red Sox\\n\"Payroll (millions)\": 173.18\\n\"Wins\": 69'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 25}, page_content='Team: Indians\\n\"Payroll (millions)\": 78.43\\n\"Wins\": 68'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 26}, page_content='Team: Twins\\n\"Payroll (millions)\": 94.08\\n\"Wins\": 66'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 27}, page_content='Team: Rockies\\n\"Payroll (millions)\": 78.06\\n\"Wins\": 64'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 28}, page_content='Team: Cubs\\n\"Payroll (millions)\": 88.19\\n\"Wins\": 61'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 29}, page_content='Team: Astros\\n\"Payroll (millions)\": 60.65\\n\"Wins\": 55')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UnstructuredCSVLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredCSVLoader(\n",
    "    file_path=\"mlb-teams-2012.csv\", mode=\"elements\"\n",
    ")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nTeam\\n\"Payroll (millions)\"\\n\"Wins\"\\n\\n\\nNationals\\n81.34\\n98\\n\\n\\nReds\\n82.20\\n97\\n\\n\\nYankees\\n197.96\\n95\\n\\n\\nGiants\\n117.62\\n94\\n\\n\\nBraves\\n83.31\\n94\\n\\n\\nAthletics\\n55.37\\n94\\n\\n\\nRangers\\n120.51\\n93\\n\\n\\nOrioles\\n81.43\\n93\\n\\n\\nRays\\n64.17\\n90\\n\\n\\nAngels\\n154.49\\n89\\n\\n\\nTigers\\n132.30\\n88\\n\\n\\nCardinals\\n110.30\\n88\\n\\n\\nDodgers\\n95.14\\n86\\n\\n\\nWhite Sox\\n96.92\\n85\\n\\n\\nBrewers\\n97.65\\n83\\n\\n\\nPhillies\\n174.54\\n81\\n\\n\\nDiamondbacks\\n74.28\\n81\\n\\n\\nPirates\\n63.43\\n79\\n\\n\\nPadres\\n55.24\\n76\\n\\n\\nMariners\\n81.97\\n75\\n\\n\\nMets\\n93.35\\n74\\n\\n\\nBlue Jays\\n75.48\\n73\\n\\n\\nRoyals\\n60.91\\n72\\n\\n\\nMarlins\\n118.07\\n69\\n\\n\\nRed Sox\\n173.18\\n69\\n\\n\\nIndians\\n78.43\\n68\\n\\n\\nTwins\\n94.08\\n66\\n\\n\\nRockies\\n78.06\\n64\\n\\n\\nCubs\\n88.19\\n61\\n\\n\\nAstros\\n60.65\\n55\\n\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <td>Team</td>\n",
      "      <td>\"Payroll (millions)\"</td>\n",
      "      <td>\"Wins\"</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Nationals</td>\n",
      "      <td>81.34</td>\n",
      "      <td>98</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Reds</td>\n",
      "      <td>82.20</td>\n",
      "      <td>97</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Yankees</td>\n",
      "      <td>197.96</td>\n",
      "      <td>95</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Giants</td>\n",
      "      <td>117.62</td>\n",
      "      <td>94</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Braves</td>\n",
      "      <td>83.31</td>\n",
      "      <td>94</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Athletics</td>\n",
      "      <td>55.37</td>\n",
      "      <td>94</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Rangers</td>\n",
      "      <td>120.51</td>\n",
      "      <td>93</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Orioles</td>\n",
      "      <td>81.43</td>\n",
      "      <td>93</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Rays</td>\n",
      "      <td>64.17</td>\n",
      "      <td>90</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Angels</td>\n",
      "      <td>154.49</td>\n",
      "      <td>89</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Tigers</td>\n",
      "      <td>132.30</td>\n",
      "      <td>88</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Cardinals</td>\n",
      "      <td>110.30</td>\n",
      "      <td>88</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Dodgers</td>\n",
      "      <td>95.14</td>\n",
      "      <td>86</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>White Sox</td>\n",
      "      <td>96.92</td>\n",
      "      <td>85</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Brewers</td>\n",
      "      <td>97.65</td>\n",
      "      <td>83</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Phillies</td>\n",
      "      <td>174.54</td>\n",
      "      <td>81</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Diamondbacks</td>\n",
      "      <td>74.28</td>\n",
      "      <td>81</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Pirates</td>\n",
      "      <td>63.43</td>\n",
      "      <td>79</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Padres</td>\n",
      "      <td>55.24</td>\n",
      "      <td>76</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Mariners</td>\n",
      "      <td>81.97</td>\n",
      "      <td>75</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Mets</td>\n",
      "      <td>93.35</td>\n",
      "      <td>74</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Blue Jays</td>\n",
      "      <td>75.48</td>\n",
      "      <td>73</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Royals</td>\n",
      "      <td>60.91</td>\n",
      "      <td>72</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Marlins</td>\n",
      "      <td>118.07</td>\n",
      "      <td>69</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Red Sox</td>\n",
      "      <td>173.18</td>\n",
      "      <td>69</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Indians</td>\n",
      "      <td>78.43</td>\n",
      "      <td>68</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Twins</td>\n",
      "      <td>94.08</td>\n",
      "      <td>66</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Rockies</td>\n",
      "      <td>78.06</td>\n",
      "      <td>64</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Cubs</td>\n",
      "      <td>88.19</td>\n",
      "      <td>61</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Astros</td>\n",
      "      <td>60.65</td>\n",
      "      <td>55</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "print(data[0].metadata[\"text_as_html\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from URL/Website files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we use `BeautifulSoup` package to load and parse a HTML or XML file. But it has some limitations.\n",
    "\n",
    "The following code is using `BeautifulSoup` to parse a website. Let's see what limitation it has.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.ibm.com/topics/langchain'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the print output, we can see that `BeautifulSoup` not only load the web content, but also a lot of HTML tags and external links, which are not necessary if we just want to load the text content of the web.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So LangChain's `WebBaseLoader` can effectively address this limitation.\n",
    "\n",
    "`WebBaseLoader` is designed to extract all text from HTML webpages and convert it into a document format suitable for further processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from single web page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://www.ibm.com/topics/langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.ibm.com/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat Is LangChain? | IBM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                        \\n\\n\\n\\n  \\n    What is LangChain?\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                                    Artificial Intelligence\\n                                \\n\\n\\n\\n\\n\\n\\n                    \\n\\n\\n\\n  \\n    31 October 2023\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Link copied\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        What is LangChain?\\r\\n    \\n\\n\\n\\nLangChain is an open source orchestration framework for the development of applications using large language models (LLMs). Available in both Python- and Javascript-based libraries, LangChain’s tools and APIs simplify the process of building LLM-driven applications like chatbots and virtual agents.\\u202f\\n\\n\\nLangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain’s module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code. This modular environment also allows for programs that use multiple LLMs: for example, an application that uses one LLM to interpret user queries and another LLM to author a response.\\nLaunched by Harrison Chase in October 2022, LangChain enjoyed a meteoric rise to prominence: as of June 2023, it was the single fastest-growing open source project on Github.1 Coinciding with the momentous launch of OpenAI’s ChatGPT the following month, LangChain has played a significant role in making generative AI more accessible to enthusiasts in the wake of its widespread popularity.\\nLangChain can facilitate most use cases for LLMs and natural language processing (NLP), like chatbots, intelligent search, question-answering, summarization services or even virtual agents capable of robotic process automation.\\n\\n\\nIntegrations with LLMs\\n\\n\\nLLMs are not standalone applications: they are pre-trained statistical models that must be paired with an application (and, in some cases, specific data sources) in order to meet their purpose.\\nFor example, Chat-GPT is not an LLM: it is a chatbot application that, depending on the version you’ve chosen, uses the GPT-3.5 or GPT-4 language model. While it’s the GPT model that interprets the user’s input and composes a natural language response, it’s the application that (among other things) provides an interface for the user to type and read and a UX design that governs the chatbot experience. Even at the enterprise level, Chat-GPT is not the only application using the GPT model: Microsoft uses GPT-4 to power Bing Chat.\\nFurthermore, though foundation models (like those powering LLMs) are pre-trained on massive datasets, they are not omniscient. If a particular task requires access to specific contextual information, like internal documentation or domain expertise, LLMs must be connected to those external data sources. Even if you simply want your model to reflect real-time awareness of current events, it requires external information: a model’s internal data is only up-to-date through the time period during which it was pre-trained.\\nLikewise, if a given generative AI task requires access to external software workflows—for example, if you wanted your virtual agent to integrate with Slack—then you will need a way to integrate the LLM with the API for that software.\\nWhile these integrations can generally be achieved with fully manual code, orchestration frameworks such as LangChain and the IBM watsonx portfolio of AI products greatly simplify the process. They also make it much easier to experiment with different LLMs to compare results, as different models can be swapped in and out with minimal changes to code.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                The latest AI News + Insights \\u2028\\n            \\n\\n                Discover expertly curated insights and news on AI, cloud and more in the weekly Think Newsletter.\\xa0\\r\\n\\n            \\n\\n\\n\\nSubscribe today\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        How does LangChain work?\\r\\n    \\n\\n\\n\\nAt LangChain’s core is a development environment that streamlines the programming of LLM applications through the use of\\xa0abstraction: the simplification of code by representing one or more complex processes as a named component that encapsulates all of its constituent steps.\\n\\n\\nAbstractions are a common element of everyday life and language. For example, “π” allows us to represent the ratio of the length of a circle’s circumference to that of its diameter without having to write out its infinite digits. Similarly, a thermostat allows us to control the temperature in our home without needing to understand the complex circuitry this entails—we only need to know how different thermostat settings translate to different temperatures.\\nLangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert programmer can finely customize an application, it empowers specialists and newcomers alike to quickly experiment and prototype.\\n\\n\\nImporting language models\\n\\n\\nNearly any LLM can be used in LangChain. Importing language models into LangChain is easy, provided you have an API key. The LLM class is designed to provide a standard interface for all models.\\nMost LLM providers will require you to create an account in order to receive an API key. Some of these APIs—particularly those for proprietary closed-source models, like those offered by OpenAI or Anthropic—may have associated costs.\\nMany open source models, like BigScience’s BLOOM, Meta AI’s LLaMa and Google’s Flan-T5, can be accessed through Hugging Face (link resides outside ibm.com). IBM watsonx, through its partnership with Hugging Face, also offers a curated suite of open source models. Creating an account with either service will allow you to generate an API key for any of the models offered by that provider.\\nLangChain is not limited to out-of-the-box foundation models: the CustomLLM class\\xa0(link resides outside ibm.com) allows for custom LLM wrappers. Likewise, you can use the IBM watsonx APIs and Python SDK, which includes a LangChain integration, to build applications in LangChain with models that you’ve already trained or fine-tuned for your specific needs using the WatsonxLLM class (and that model’s specific project ID).\\n\\n\\nPrompt templates\\n\\n\\nPrompts are the instructions given to an LLM. The “art” of composing prompts that effectively provide the context necessary for the LLM to interpret input and structure output in the way most useful to you is often called prompt engineering.\\nThe PromptTemplate class in LangChain formalizes the composition of prompts without the need to manually hard code context and queries. Important elements of a prompt are likewise entered as formal classes, like input_variables. A prompt template can thus contain and reproduce context, instructions (like “do not use technical terms”), a set of examples to guide its responses (in what is called “few-shot prompting”), a specified output format or a standardized question to be answered.\\u202fYou can save and name an effectively structured prompt template and easily reuse it as needed.\\nThough these elements can all be manually coded, PromptTemplate modules empower smooth integration with other LangChain features, like the eponymous chains.\\n\\n\\nChains\\n\\n\\nAs its name implies, chains are the core of LangChain’s workflows. They combine LLMs with other components, creating applications by executing a sequence of functions.\\nThe most basic chain is LLMChain. It simply calls a model and prompt template for that model. For example, imagine you saved a prompt as “ExamplePrompt” and wanted to run it against Flan-T5. You can import LLMChain from langchain.chains, then define chain_example = LLMChain(llm = flan-t5, prompt = ExamplePrompt). To run the chain for a given input, you simply call chain_example.run(“input”).\\nTo use the output of one function as the input for the next function, you can use SimpleSequentialChain. Each function could utilize different prompts, different tools, different parameters or even different models, depending on your specific needs.\\n\\n\\nIndexes\\n\\n\\nTo achieve certain tasks, LLMs will need access to specific external data sources not included in its training dataset, such as internal documents, emails or datasets. LangChain collectively refers to such external documentation as “indexes”.\\n\\n\\nDocument loaders\\n\\n\\nLangChain offers\\xa0a wide variety of document loaders for third party applications\\xa0(link resides outside ibm.com). This allows for easy importation of data from sources like file storage services (like Dropbox, Google Drive and Microsoft OneDrive), web content (like YouTube, PubMed or specific URLs), collaboration tools (like Airtable, Trello, Figma and Notion), databases (like Pandas, MongoDB and Microsoft), among many others.\\n\\n\\nVector databases\\n\\n\\nUnlike “traditional” structured databases,\\xa0vector databases\\xa0represent data points by converting them into\\xa0vector embeddings: numerical representations in the form of vectors with a fixed number of dimensions, often clustering related data points using\\xa0unsupervised learning methods. This enables low latency queries, even for massive datasets, which greatly increases efficiency. Vector embeddings also store each vector’s metadata, further enhancing search possibilities.\\nLangChain provides integrations for over 25 different embedding methods, as well as for over 50 different vector stores (both cloud-hosted and local).\\n\\n\\nText splitters\\xa0\\n\\n\\nTo increase speed and reduce computational demands, it’s often wise to split large text documents into smaller pieces. LangChain’s\\xa0TextSplitters\\xa0split text up into small, semantically meaningful chunks that can then be combined using methods and parameters of your choosing.\\n\\n\\nRetrieval\\n\\n\\nOnce external sources of knowledge have been connected, the model must be able to quickly retrieve and integrate relevant information as needed. Like watsonx, LangChain offers\\xa0retrieval augmented generation (RAG):\\xa0its\\xa0retriever\\xa0modules accept a string query as an input and return a list of\\xa0Document’s as output.\\n\\n\\nMemory\\n\\n\\nLLMs, by default, do not have any long-term memory of prior conversations (unless that chat history is used as input for a query). LangChain solves this problem with simple utilities for adding memory to a system, with options ranging from retaining the entirety of all conversations to retaining a summarization of the conversation thus far to retaining the n\\xa0most recent exchanges.\\n\\n\\nAgents\\n\\n\\nLangChain agents can use a given language model as a “reasoning engine” to determine which actions to take. When building a chain for an agent, inputs include:\\na list of available tools to be leveraged.user input (like prompts and queries).any relevant previously executed steps.\\n\\n\\nTools\\n\\n\\nDespite their heralded power and versatility, LLMs have important limitations: namely, a lack of up-to-date information, a lack of domain-specific expertise and a general difficulty with math.\\nLangChain tools\\xa0(link resides outside ibm.com) are a set of functions that empower LangChain agents to interact with real-world information in order to expand or improve the services it can provide. Examples of prominent LangChain tools include:\\n\\nWolfram Alpha: provides access to powerful computational and data visualization functions, enabling sophisticated mathematical capabilities.\\nGoogle Search: provides access to Google Search, equipping applications and agents with real-time information.\\nOpenWeatherMap: fetches weather information.\\nWikipedia: provides efficient access to information from Wikipedia articles.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n      AI Academy\\n  \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n\\n  \\n    Why foundation models are a paradigm shift for AI\\n\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\nLearn about a new class of flexible, reusable AI models that can unlock new revenue, reduce costs and increase productivity, then use our guidebook to dive deeper.\\n\\n\\n\\n\\nGo to episode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        LangSmith\\r\\n    \\n\\n\\n\\nReleased in the fall of 2023, LangSmith aims to bridge the gap between the accessible prototyping capabilities that brought LangChain to prominence and building production-quality LLM applications.\\nLangSmith provides tools to monitor, evaluate and debug applications, including the ability to automatically trace all model calls to spot errors and test performance under different model configurations. This visibility aims to empower more robust, cost-efficient applications.\\n\\n\\n\\n\\r\\n        Getting started with LangChain\\r\\n    \\n\\n\\n\\nLangChain is open source and free to use: source code is\\xa0available for download on Github\\xa0(link resides outside ibm.com).\\nLangChain can also be installed on Python with a simple pip command:\\xa0pip install langchain.\\u202fTo install all LangChain dependencies (rather than only those you find necessary), you can run the command\\xa0pip install langchain[all].\\nMany step-by-step tutorials are available from both the greater LangChain community ecosystem and the official documentation at\\xa0docs.langchain.com\\xa0(link resides outside ibm.com).\\n\\n\\n\\r\\n        LangChain use cases\\r\\n    \\n\\n\\n\\nApplications made with LangChain provide great utility for a variety of use cases, from straightforward question-answering and text generation tasks to more complex solutions that use an LLM as a “reasoning engine.”\\n\\n\\nChatbots: Chatbots are among the most intuitive uses of LLMs. LangChain can be used to provide proper context for the specific use of a chatbot, and to integrate chatbots into existing communication channels and workflows with their own APIs.Summarization: Language models can be tasked with summarizing many types of text, from breaking down complex academic articles and transcripts to providing a digest of incoming emails.Question answering: Using specific documents or specialized knowledge bases (like Wolfram, arXiv or PubMed), LLMs can retrieve relevant information from storage and articulate helpful answers). If fine-tuned or properly prompted, some LLMs can answer many questions even without external information.Data augmentation: LLMs can be used to generate synthetic data for use in machine learning. For example, an LLM can be trained to generate additional data samples that closely resemble the data points in a training dataset.Virtual agents: Integrated with the right workflows, LangChain’s Agent modules can use an LLM to autonomously determine next steps and take action using robotic process automation (RPA).\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Ebook\\n        \\n\\n            How to choose the right foundation model\\n        \\nLearn how to choose the right approach in preparing datasets and employing foundation models.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\n\\n     \\n    Related solutions\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Foundation models\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nExplore the IBM library of foundation models on the watsonx platform to scale generative AI for your business with confidence.\\n\\n\\n\\nDiscover watsonx.ai\\n            \\n        \\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Artificial intelligence solutions\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nPut AI to work in your business with IBM's industry-leading AI expertise and portfolio of solutions at your side.\\n\\n\\n\\nExplore AI solutions\\n            \\n        \\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    AI consulting and services\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nReinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.\\n\\n\\n\\nExplore AI services\\n            \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n\\n  \\n    Resources\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            AI models\\n        \\n\\n\\n            Explore IBM Granite\\n        \\nIBM® Granite™ is our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.\\n\\nMeet Granite\\n\\n\\n\\n\\n\\n\\n\\n            Ebook\\n        \\n\\n\\n            How to choose the right foundation model\\n        \\nLearn how to select the most suitable AI foundation model for your use case.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n            Article\\n        \\n\\n\\n            Discover the power of LLMs\\n        \\nDive into IBM Developer articles, blogs and tutorials to deepen your knowledge of LLMs.\\n\\nExplore the articles\\n\\n\\n\\n\\n\\n\\n\\n            Guide\\n        \\n\\n\\n            The CEO's guide to model optimization\\n        \\nLearn how to continually push teams to improve model performance and outpace the competition by using the latest AI techniques and infrastructure.\\n\\nRead the guide\\n\\n\\n\\n\\n\\n\\n\\n            Report\\n        \\n\\n\\n            A differentiated approach to AI foundation models\\n        \\nExplore the value of enterprise-grade foundation models that\\r\\nprovide trust, performance and cost-effective benefits to\\r\\nall industries.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n            Ebook\\n        \\n\\n\\n            Unlock the Power of Generative AI + ML\\n        \\nLearn how to incorporate generative AI, machine learning and foundation models into your business operations for improved performance.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n            Report\\n        \\n\\n\\n            AI in Action 2024\\n        \\nWe surveyed 2,000 organizations about their AI initiatives to discover what's working, what's not and how you can get ahead.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTake the next step\\n\\n\\n\\n\\nExplore the IBM library of foundation models on the IBM watsonx platform to scale generative AI for your business with confidence.\\n\\n\\n\\n\\n\\nExplore watsonx.ai\\n\\n\\n\\nExplore AI solutions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            \\n\\n\\n\\n  \\n    Footnotes\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                        \\n\\n\\n\\n\\n\\n1\\xa0The fastest-growing open-source startups in Q2 2023\\xa0(link resides outside ibm.com), Runa Capital, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from multiple web pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.ibm.com/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat Is LangChain? | IBM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                        \\n\\n\\n\\n  \\n    What is LangChain?\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                                    Artificial Intelligence\\n                                \\n\\n\\n\\n\\n\\n\\n                    \\n\\n\\n\\n  \\n    31 October 2023\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Link copied\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        What is LangChain?\\r\\n    \\n\\n\\n\\nLangChain is an open source orchestration framework for the development of applications using large language models (LLMs). Available in both Python- and Javascript-based libraries, LangChain’s tools and APIs simplify the process of building LLM-driven applications like chatbots and virtual agents.\\u202f\\n\\n\\nLangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain’s module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code. This modular environment also allows for programs that use multiple LLMs: for example, an application that uses one LLM to interpret user queries and another LLM to author a response.\\nLaunched by Harrison Chase in October 2022, LangChain enjoyed a meteoric rise to prominence: as of June 2023, it was the single fastest-growing open source project on Github.1 Coinciding with the momentous launch of OpenAI’s ChatGPT the following month, LangChain has played a significant role in making generative AI more accessible to enthusiasts in the wake of its widespread popularity.\\nLangChain can facilitate most use cases for LLMs and natural language processing (NLP), like chatbots, intelligent search, question-answering, summarization services or even virtual agents capable of robotic process automation.\\n\\n\\nIntegrations with LLMs\\n\\n\\nLLMs are not standalone applications: they are pre-trained statistical models that must be paired with an application (and, in some cases, specific data sources) in order to meet their purpose.\\nFor example, Chat-GPT is not an LLM: it is a chatbot application that, depending on the version you’ve chosen, uses the GPT-3.5 or GPT-4 language model. While it’s the GPT model that interprets the user’s input and composes a natural language response, it’s the application that (among other things) provides an interface for the user to type and read and a UX design that governs the chatbot experience. Even at the enterprise level, Chat-GPT is not the only application using the GPT model: Microsoft uses GPT-4 to power Bing Chat.\\nFurthermore, though foundation models (like those powering LLMs) are pre-trained on massive datasets, they are not omniscient. If a particular task requires access to specific contextual information, like internal documentation or domain expertise, LLMs must be connected to those external data sources. Even if you simply want your model to reflect real-time awareness of current events, it requires external information: a model’s internal data is only up-to-date through the time period during which it was pre-trained.\\nLikewise, if a given generative AI task requires access to external software workflows—for example, if you wanted your virtual agent to integrate with Slack—then you will need a way to integrate the LLM with the API for that software.\\nWhile these integrations can generally be achieved with fully manual code, orchestration frameworks such as LangChain and the IBM watsonx portfolio of AI products greatly simplify the process. They also make it much easier to experiment with different LLMs to compare results, as different models can be swapped in and out with minimal changes to code.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                The latest AI News + Insights \\u2028\\n            \\n\\n                Discover expertly curated insights and news on AI, cloud and more in the weekly Think Newsletter.\\xa0\\r\\n\\n            \\n\\n\\n\\nSubscribe today\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        How does LangChain work?\\r\\n    \\n\\n\\n\\nAt LangChain’s core is a development environment that streamlines the programming of LLM applications through the use of\\xa0abstraction: the simplification of code by representing one or more complex processes as a named component that encapsulates all of its constituent steps.\\n\\n\\nAbstractions are a common element of everyday life and language. For example, “π” allows us to represent the ratio of the length of a circle’s circumference to that of its diameter without having to write out its infinite digits. Similarly, a thermostat allows us to control the temperature in our home without needing to understand the complex circuitry this entails—we only need to know how different thermostat settings translate to different temperatures.\\nLangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert programmer can finely customize an application, it empowers specialists and newcomers alike to quickly experiment and prototype.\\n\\n\\nImporting language models\\n\\n\\nNearly any LLM can be used in LangChain. Importing language models into LangChain is easy, provided you have an API key. The LLM class is designed to provide a standard interface for all models.\\nMost LLM providers will require you to create an account in order to receive an API key. Some of these APIs—particularly those for proprietary closed-source models, like those offered by OpenAI or Anthropic—may have associated costs.\\nMany open source models, like BigScience’s BLOOM, Meta AI’s LLaMa and Google’s Flan-T5, can be accessed through Hugging Face (link resides outside ibm.com). IBM watsonx, through its partnership with Hugging Face, also offers a curated suite of open source models. Creating an account with either service will allow you to generate an API key for any of the models offered by that provider.\\nLangChain is not limited to out-of-the-box foundation models: the CustomLLM class\\xa0(link resides outside ibm.com) allows for custom LLM wrappers. Likewise, you can use the IBM watsonx APIs and Python SDK, which includes a LangChain integration, to build applications in LangChain with models that you’ve already trained or fine-tuned for your specific needs using the WatsonxLLM class (and that model’s specific project ID).\\n\\n\\nPrompt templates\\n\\n\\nPrompts are the instructions given to an LLM. The “art” of composing prompts that effectively provide the context necessary for the LLM to interpret input and structure output in the way most useful to you is often called prompt engineering.\\nThe PromptTemplate class in LangChain formalizes the composition of prompts without the need to manually hard code context and queries. Important elements of a prompt are likewise entered as formal classes, like input_variables. A prompt template can thus contain and reproduce context, instructions (like “do not use technical terms”), a set of examples to guide its responses (in what is called “few-shot prompting”), a specified output format or a standardized question to be answered.\\u202fYou can save and name an effectively structured prompt template and easily reuse it as needed.\\nThough these elements can all be manually coded, PromptTemplate modules empower smooth integration with other LangChain features, like the eponymous chains.\\n\\n\\nChains\\n\\n\\nAs its name implies, chains are the core of LangChain’s workflows. They combine LLMs with other components, creating applications by executing a sequence of functions.\\nThe most basic chain is LLMChain. It simply calls a model and prompt template for that model. For example, imagine you saved a prompt as “ExamplePrompt” and wanted to run it against Flan-T5. You can import LLMChain from langchain.chains, then define chain_example = LLMChain(llm = flan-t5, prompt = ExamplePrompt). To run the chain for a given input, you simply call chain_example.run(“input”).\\nTo use the output of one function as the input for the next function, you can use SimpleSequentialChain. Each function could utilize different prompts, different tools, different parameters or even different models, depending on your specific needs.\\n\\n\\nIndexes\\n\\n\\nTo achieve certain tasks, LLMs will need access to specific external data sources not included in its training dataset, such as internal documents, emails or datasets. LangChain collectively refers to such external documentation as “indexes”.\\n\\n\\nDocument loaders\\n\\n\\nLangChain offers\\xa0a wide variety of document loaders for third party applications\\xa0(link resides outside ibm.com). This allows for easy importation of data from sources like file storage services (like Dropbox, Google Drive and Microsoft OneDrive), web content (like YouTube, PubMed or specific URLs), collaboration tools (like Airtable, Trello, Figma and Notion), databases (like Pandas, MongoDB and Microsoft), among many others.\\n\\n\\nVector databases\\n\\n\\nUnlike “traditional” structured databases,\\xa0vector databases\\xa0represent data points by converting them into\\xa0vector embeddings: numerical representations in the form of vectors with a fixed number of dimensions, often clustering related data points using\\xa0unsupervised learning methods. This enables low latency queries, even for massive datasets, which greatly increases efficiency. Vector embeddings also store each vector’s metadata, further enhancing search possibilities.\\nLangChain provides integrations for over 25 different embedding methods, as well as for over 50 different vector stores (both cloud-hosted and local).\\n\\n\\nText splitters\\xa0\\n\\n\\nTo increase speed and reduce computational demands, it’s often wise to split large text documents into smaller pieces. LangChain’s\\xa0TextSplitters\\xa0split text up into small, semantically meaningful chunks that can then be combined using methods and parameters of your choosing.\\n\\n\\nRetrieval\\n\\n\\nOnce external sources of knowledge have been connected, the model must be able to quickly retrieve and integrate relevant information as needed. Like watsonx, LangChain offers\\xa0retrieval augmented generation (RAG):\\xa0its\\xa0retriever\\xa0modules accept a string query as an input and return a list of\\xa0Document’s as output.\\n\\n\\nMemory\\n\\n\\nLLMs, by default, do not have any long-term memory of prior conversations (unless that chat history is used as input for a query). LangChain solves this problem with simple utilities for adding memory to a system, with options ranging from retaining the entirety of all conversations to retaining a summarization of the conversation thus far to retaining the n\\xa0most recent exchanges.\\n\\n\\nAgents\\n\\n\\nLangChain agents can use a given language model as a “reasoning engine” to determine which actions to take. When building a chain for an agent, inputs include:\\na list of available tools to be leveraged.user input (like prompts and queries).any relevant previously executed steps.\\n\\n\\nTools\\n\\n\\nDespite their heralded power and versatility, LLMs have important limitations: namely, a lack of up-to-date information, a lack of domain-specific expertise and a general difficulty with math.\\nLangChain tools\\xa0(link resides outside ibm.com) are a set of functions that empower LangChain agents to interact with real-world information in order to expand or improve the services it can provide. Examples of prominent LangChain tools include:\\n\\nWolfram Alpha: provides access to powerful computational and data visualization functions, enabling sophisticated mathematical capabilities.\\nGoogle Search: provides access to Google Search, equipping applications and agents with real-time information.\\nOpenWeatherMap: fetches weather information.\\nWikipedia: provides efficient access to information from Wikipedia articles.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n      AI Academy\\n  \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n\\n  \\n    Why foundation models are a paradigm shift for AI\\n\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\nLearn about a new class of flexible, reusable AI models that can unlock new revenue, reduce costs and increase productivity, then use our guidebook to dive deeper.\\n\\n\\n\\n\\nGo to episode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        LangSmith\\r\\n    \\n\\n\\n\\nReleased in the fall of 2023, LangSmith aims to bridge the gap between the accessible prototyping capabilities that brought LangChain to prominence and building production-quality LLM applications.\\nLangSmith provides tools to monitor, evaluate and debug applications, including the ability to automatically trace all model calls to spot errors and test performance under different model configurations. This visibility aims to empower more robust, cost-efficient applications.\\n\\n\\n\\n\\r\\n        Getting started with LangChain\\r\\n    \\n\\n\\n\\nLangChain is open source and free to use: source code is\\xa0available for download on Github\\xa0(link resides outside ibm.com).\\nLangChain can also be installed on Python with a simple pip command:\\xa0pip install langchain.\\u202fTo install all LangChain dependencies (rather than only those you find necessary), you can run the command\\xa0pip install langchain[all].\\nMany step-by-step tutorials are available from both the greater LangChain community ecosystem and the official documentation at\\xa0docs.langchain.com\\xa0(link resides outside ibm.com).\\n\\n\\n\\r\\n        LangChain use cases\\r\\n    \\n\\n\\n\\nApplications made with LangChain provide great utility for a variety of use cases, from straightforward question-answering and text generation tasks to more complex solutions that use an LLM as a “reasoning engine.”\\n\\n\\nChatbots: Chatbots are among the most intuitive uses of LLMs. LangChain can be used to provide proper context for the specific use of a chatbot, and to integrate chatbots into existing communication channels and workflows with their own APIs.Summarization: Language models can be tasked with summarizing many types of text, from breaking down complex academic articles and transcripts to providing a digest of incoming emails.Question answering: Using specific documents or specialized knowledge bases (like Wolfram, arXiv or PubMed), LLMs can retrieve relevant information from storage and articulate helpful answers). If fine-tuned or properly prompted, some LLMs can answer many questions even without external information.Data augmentation: LLMs can be used to generate synthetic data for use in machine learning. For example, an LLM can be trained to generate additional data samples that closely resemble the data points in a training dataset.Virtual agents: Integrated with the right workflows, LangChain’s Agent modules can use an LLM to autonomously determine next steps and take action using robotic process automation (RPA).\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Ebook\\n        \\n\\n            How to choose the right foundation model\\n        \\nLearn how to choose the right approach in preparing datasets and employing foundation models.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\n\\n     \\n    Related solutions\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Foundation models\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nExplore the IBM library of foundation models on the watsonx platform to scale generative AI for your business with confidence.\\n\\n\\n\\nDiscover watsonx.ai\\n            \\n        \\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Artificial intelligence solutions\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nPut AI to work in your business with IBM's industry-leading AI expertise and portfolio of solutions at your side.\\n\\n\\n\\nExplore AI solutions\\n            \\n        \\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    AI consulting and services\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nReinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.\\n\\n\\n\\nExplore AI services\\n            \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n\\n  \\n    Resources\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            AI models\\n        \\n\\n\\n            Explore IBM Granite\\n        \\nIBM® Granite™ is our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.\\n\\nMeet Granite\\n\\n\\n\\n\\n\\n\\n\\n            Ebook\\n        \\n\\n\\n            How to choose the right foundation model\\n        \\nLearn how to select the most suitable AI foundation model for your use case.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n            Article\\n        \\n\\n\\n            Discover the power of LLMs\\n        \\nDive into IBM Developer articles, blogs and tutorials to deepen your knowledge of LLMs.\\n\\nExplore the articles\\n\\n\\n\\n\\n\\n\\n\\n            Guide\\n        \\n\\n\\n            The CEO's guide to model optimization\\n        \\nLearn how to continually push teams to improve model performance and outpace the competition by using the latest AI techniques and infrastructure.\\n\\nRead the guide\\n\\n\\n\\n\\n\\n\\n\\n            Report\\n        \\n\\n\\n            A differentiated approach to AI foundation models\\n        \\nExplore the value of enterprise-grade foundation models that\\r\\nprovide trust, performance and cost-effective benefits to\\r\\nall industries.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n            Ebook\\n        \\n\\n\\n            Unlock the Power of Generative AI + ML\\n        \\nLearn how to incorporate generative AI, machine learning and foundation models into your business operations for improved performance.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n            Report\\n        \\n\\n\\n            AI in Action 2024\\n        \\nWe surveyed 2,000 organizations about their AI initiatives to discover what's working, what's not and how you can get ahead.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTake the next step\\n\\n\\n\\n\\nExplore the IBM library of foundation models on the IBM watsonx platform to scale generative AI for your business with confidence.\\n\\n\\n\\n\\n\\nExplore watsonx.ai\\n\\n\\n\\nExplore AI solutions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            \\n\\n\\n\\n  \\n    Footnotes\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                        \\n\\n\\n\\n\\n\\n1\\xa0The fastest-growing open-source startups in Q2 2023\\xa0(link resides outside ibm.com), Runa Capital, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"),\n",
       " Document(metadata={'source': 'https://www.redhat.com/en/topics/ai/what-is-instructlab', 'title': 'What is InstructLab?', 'description': 'InstructLab is an open source project for enhancing large language models (LLMs).', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\nWhat is InstructLab?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to contentFeatured linksSupportDocumentationConsoleDevelopersStart a trial\\n                All Red HatFor customersCustomer supportSubscription managementSupport casesRed Hat Ecosystem CatalogFind a partnerFor partnersPartner portalPartner supportBecome a partner Try, buy, & sellRed Hat MarketplaceRed Hat StoreContact salesStart a trialLearning resourcesDocumentationTraining and certification Hybrid cloud learning hubInteractive labsLearning communityRed Hat TVOpen source communitiesGlobal advocacyHow we contributeRed HatProductsSolutionsTraining & servicesResourcesPartnersAboutExplore morePlatform productsRed Hat Enterprise LinuxA flexible, stable operating system to support hybrid cloud innovation.\\n                      Red Hat OpenShiftA container platform to build, modernize, and deploy applications at scale.\\n                      Red Hat Ansible Automation PlatformNew versionA foundation for implementing enterprise-wide automation.\\n                      Try & buyStart a trialAssess a product with a no-cost trial.\\n                      Buy onlineBuy select products and services in the Red Hat Store.\\n                      Integrate with major cloud providersBuy Red Hat solutions using committed spend from providers, including:\\n                          Featured\\n      Red Hat Enterprise Linux AI\\n    \\n      Red Hat OpenShift AI\\n    \\n      Red Hat OpenShift Virtualization\\n    \\n      Red Hat OpenShift Service on AWS\\n    \\n      Microsoft Azure Red Hat OpenShift\\n    See all products\\n              Application platformSimplify the way you build, deploy, manage, and secure apps across the hybrid cloud.\\n                      Artificial intelligenceBuild, deploy, and monitor AI models and apps with Red Hat\\'s open source platforms.\\n                      Edge computingDeploy workloads closer to the source with security-focused edge technology.\\n                      IT automationUnite disparate tech, teams, and environments with 1 comprehensive automation platform.\\n                      Linux standardizationGet consistency across operating environments with an open, flexible infrastructure.\\n                      SecurityDeliver software using trusted platforms and real-time security scanning and remediation.\\n                      VirtualizationModernize operations using a single platform for virtualized and containerized workloads.\\n                      By industry\\n      Automotive\\n    \\n      Financial services\\n    \\n      Healthcare\\n    \\n      Industrial sector\\n    \\n      Media and entertainment\\n    \\n      Public sector\\n    \\n      Telecommunications\\n    Explore solutions\\n              \\n              Services\\n          \\n      Consulting\\n    \\n      Services for AI\\n    \\n      Technical Account Management\\n    \\n              Training & certification\\n          \\n      All courses and exams\\n    \\n      All certifications\\n    \\n      Verify a certification\\n    \\n      Skills assessment\\n    \\n      Learning subscription\\n    \\n      Learning community\\n    \\n      Red Hat Academy\\n    \\n      FAQs\\n    \\n      Connect with learning experts\\n    Featured\\n      Ansible Basics: Automation Technical Overview (No cost)\\n    \\n      Containers, Kubernetes and Red Hat OpenShift Technical Overview (No cost)\\n    \\n      Red Hat Enterprise Linux Technical Overview (No cost)\\n    \\n      Red Hat Certified System Administrator exam\\n    \\n      Red Hat System Administration I\\n    Explore services\\n              Topics\\n      AI\\n    \\n      Application modernization\\n    \\n      Automation\\n    \\n      Cloud computing\\n    \\n      Cloud-native applications\\n    \\n      Containers\\n    \\n      DevOps\\n    \\n      Edge computing\\n    \\n      Linux\\n    \\n      Virtualization\\n    \\n      See all topics\\n    Articles\\n      What is InstructLab?\\n    \\n      What are cloud services?\\n    \\n      What is edge computing?\\n    \\n      What is hybrid cloud?\\n    \\n      Why build a Red Hat cloud?\\n    \\n      Cloud vs. edge\\n    \\n      Red Hat OpenShift vs. Kubernetes\\n    \\n      Learning Ansible basics\\n    \\n      What is Linux?\\n    More to explore\\n      Blog\\n    \\n      Customer success stories\\n    \\n      Events and webinars\\n    \\n      Newsroom\\n    \\n      Podcasts and video series\\n    \\n      Documentation\\n    \\n      Resource library\\n    \\n      Training and certification\\n    Explore resources\\n              For customers\\n      Our partners\\n    \\n      Red Hat Ecosystem Catalog\\n    \\n      Find a partner\\n    For partners\\n      Partner Connect\\n    \\n      Become a partner\\n    \\n      Training\\n    \\n      Support\\n    \\n      Access the partner portal\\n    About us\\n      Our company\\n    \\n      How we work\\n    \\n      Our social impact\\n    \\n      Development model\\n    \\n      Subscription model\\n    \\n      Product support\\n    Open source\\n      Open source commitments\\n    \\n      How we contribute\\n    \\n      Red Hat on GitHub\\n    Company details\\n      Analyst relations\\n    \\n      Blog\\n    \\n      Locations\\n    \\n      Newsroom\\n    Explore Red Hat\\n              Contact us\\n              For customersCustomer supportSubscription managementSupport casesRed Hat Ecosystem CatalogFind a partnerFor partnersPartner portalPartner supportBecome a partner Try, buy, & sellRed Hat MarketplaceRed Hat StoreContact salesStart a trialLearning resourcesDocumentationTraining and certification Hybrid cloud learning hubInteractive labsLearning communityRed Hat TVOpen source communitiesGlobal advocacyHow we contribute\\n      For you\\n      NewRecommendationsAs you browse redhat.com, we\\'ll recommend resources you may like. For now, try these.All Red Hat productsTech topicsRed Hat resourcesSupportDocumentationConsoleDevelopersStart a trialContactSelect a language简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañol\\n        Contact us\\n    \\n      English\\n    Select a language简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañolRed HatProductsSolutionsTraining & servicesResourcesPartnersAboutMenu\\n        Search\\n      \\n        For you\\n      \\n        Contact us\\n      \\n        English\\n      \\n        Log in\\n      ProductsSolutionsTraining & servicesResourcesPartnersAboutContact usSelect a language简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañol\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                                Topics                                                            Open source                            \\n                    What is InstructLab?                     \\n\\n\\nWhat is InstructLab?\\n\\n\\n\\n\\nPublished  May 7, 2024•5-minute readCopy URL\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nOverviewInstructLab is an open source project for enhancing\\xa0large language models (LLMs) used in\\xa0generative artificial intelligence (gen AI) applications. Created by IBM and Red Hat, the InstructLab community project provides a cost-effective solution for improving the alignment of LLMs and opens the doors for those with minimal machine learning experience to contribute.Join the InstructLab community \\nWhat does InstructLab do?LLMs can power a range of useful applications like chatbots and coding assistants. These LLMs can be proprietary (such as OpenAI’s GPT models and Anthropic’s Claude models) or offer varying degrees of openness around pretraining data and usage restrictions (such as Meta’s Llama models, Mistral AI’s Mistral models, and IBM’s Granite models).AI practitioners often need to adapt a pretrained LLM to suit a particular business purpose. But there are limits to the ways you can modify an LLM:Fine-tuning an LLM to understand a specific area of knowledge or skills typically involves forking an existing open model, then running expensive, resource-intensive training.There’s no way to incorporate improvements back to the upstream project, and thus no way for models to continuously improve from community contributions.LLM refinements have typically required large amounts of human-generated data, which can be time-consuming and expensive to get.InstructLab follows an approach that punches through those limitations. It can enhance an LLM using far less human-generated information and far fewer computing resources than are typically used to retrain a model. And it makes it possible for upstream contributions to continuously make the model better.InstructLab is named after and based on IBM Research’s work on Large-scale Alignment for chatBots, abbreviated as LAB. The LAB method is described in a\\xa02024 research paper by members of the MIT-IBM Watson AI Lab and IBM Research.InstructLab is not model-specific. It can provide supplemental skills and knowledge fine-tuning to an LLM of your choice. This “tree of skills and knowledge” improves continuously from community contributions and can be applied to support regular builds of an enhanced LLM. InstructLab maintains an\\xa0enhanced version of IBM Granite. Two other lab-enhanced models released by IBM include\\xa0Labradorite, which is derived from Llama 2, and\\xa0Merlinite, which is derived from Mistral. The InstructLab project prioritizes fast iteration and intends to retrain models on a regular basis. Organizations can also use the InstructLab model alignment tools to train their own private LLMs with their own proprietary skills and knowledge. \\n\\nRed Hat resourcesKeep reading\\n\\n\\nHow does InstructLab work?The LAB method consists of 3 components:Taxonomy-driven data curation. Taxonomy is a set of diverse training data curated by humans as examples of new knowledge and skills for the model.Large-scale synthetic data generation. The model is then used to generate new examples based on the seed training data. Recognizing that synthetic data can vary in quality, the LAB method adds an automated step to refine the example answers, making sure they’re grounded and safe.Iterative, large-scale alignment tuning. Finally, the model is retrained based on the set of synthetic data. The LAB method includes 2 tuning phases: knowledge tuning, followed by skill tuning.The contributions of data from the community can lead to regular iterative builds of enhanced LLMs, each made better by the tree of skills generated from community contributions. \\nHow is InstructLab different from other methods of training an LLM?Let’s compare InstructLab to the other steps in creating and improving an LLM.PretrainingDuring pretraining, an LLM is trained to predict the next token using trillions of tokens of unlabeled data. This gets really expensive, sometimes requiring thousands of GPUs and months of time. Pretraining a highly capable LLM is only possible for organizations with significant resources.Alignment tuningAfter pretraining, LLMs undergo alignment tuning to make the model’s answers as accurate and useful as possible. The 1st step in alignment tuning is typically instruction tuning, in which a model is trained directly on specific tasks of interest. Next is preference tuning, which can include reinforcement learning from human feedback (RLHF). In this step, humans test the model and rate its output, noting if the model’s answers are preferred or unpreferred. An RLHF process may include multiple rounds of feedback and refinement to optimize a model.Researchers have found that the amount of feedback at this alignment tuning stage can be much smaller than the initial set of training data―tens of thousands of human annotations, compared to the trillions of tokens of data required for pretraining―and still unlock latent capabilities of the model.InstructLabThe LAB method emerged from the idea that it should be possible to realize the benefits of model alignment from an even smaller set of human-generated data. An AI model can use a handful of human examples to generate a large amount of synthetic data―then refine that list for quality―and use that high-quality synthetic data set for further tuning and training. In contrast to instruction tuning, which typically need thousands of examples of human feedback, LAB can make a model significantly better using relatively few examples provided by humans.How is InstructLab different from retrieval-augmented generation (RAG)?The short answer is InstructLab and retrieval-augmented generation (RAG) solve different problems.RAG is a cost-efficient method for supplementing an LLM with domain-specific knowledge that wasn’t part of its pretraining. RAG makes it possible for a chatbot to accurately answer questions related to a specific field or business without retraining the model. Knowledge documents are stored in a vector database, then retrieved in chunks and sent to the model as part of user queries. This is helpful for anyone who wants to add proprietary data to an LLM without giving up control of their information, or who needs an LLM to access timely information.\\xa0This is in contrast to the InstructLab method, which sources end-user contributions to support regular builds of an enhanced version of an LLM. InstructLab helps add knowledge and unlock new skills of an LLM.It’s possible to \"supercharge\" a RAG process by using the RAG technique on an InstructLab-tuned model.Learn more about RAG\\xa0 \\nWhat are the components of the InstructLab project?InstructLab is composed of several projects.TaxonomyInstructLab is driven by taxonomies, which are largely created manually and with care. InstructLab contains a taxonomy tree that lets users create models tuned with human-provided data, which is then enhanced with synthetic data generation.Command-line interface (CLI)The InstructLab CLI lets contributors test their contributions using their laptop or workstation. Community members can use the InstructLab technique to generate a low-fidelity approximation of synthetic data generation and model-instruction tuning without access to specialized hardware.Model training infrastructureFinally, there’s the process of creating the enhanced LLMs. It takes GPU-intensive infrastructure to regularly retrain models based on new contributions from the community. IBM donates and maintains the infrastructure necessary to frequently retrain the InstructLab project’s enhanced models.Dig deeper into AI infrastructure \\nDiscover Red Hat Enterprise Linux AIWhen you’re ready to bring AI to the enterprise, Red Hat® Enterprise Linux® AI brings together the Granite family of open source-licensed LLMs, InstructLab model alignment tools, a bootable image of Red Hat Enterprise Linux, enterprise-grade technical support, and model intellectual property indemnification.Red Hat Enterprise Linux is the world’s leading enterprise Linux platform, certified on hundreds of clouds and with thousands of hardware and software vendors. With the technological foundation of Linux, containers, and automation, Red Hat’s open hybrid cloud strategy gives you the flexibility to run your AI applications anywhere you need them.Red Hat Enterprise Linux AI and the InstructLab project further deliver on this vision, breaking down the cost and resource barriers to experimenting with and building AI models while providing the tools, data, and concepts needed to fuel the next wave of intelligent workloads.Explore Red Hat Enterprise Linux AI \\n\\n\\n\\n\\n\\n\\n\\nHub\\n \\n\\n\\n\\n\\n\\n\\nThe official Red Hat blog\\n\\n\\n\\nGet the latest information about our ecosystem of customers, partners, and communities.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nKeep reading\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nAll Red Hat product trialsOur no-cost product trials help you gain hands-on experience, prepare for a certification, or assess if a product is right for your organization.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nKeep reading\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nKeep reading\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is KVM?\\n\\n\\n\\n        Kernel-based virtual machines (KVM) are an open source virtualization technology that turns Linux into a hypervisor.\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead the article\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Podman Desktop?\\n\\n\\n\\n        Podman Desktop is a free, open source tool that simplifies working with containers in a local developer environment.\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead the article\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is CentOS Stream?\\n\\n\\n\\n        CentOS Stream is a Linux® development platform where open source community members can contribute to Red Hat® Enterprise Linux in tandem with Red Hat developers. \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead the article\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen source resources\\n\\n\\n\\n\\n\\n\\nRelated content\\n\\n\\n\\n\\n\\n\\nBlog post\\n\\nTop 10 blog posts about the open source community in 2024\\n\\n\\n\\nE-book\\n\\nMake the most of your Red Hat Enterprise Linux experience\\n\\n\\n\\nE-book\\n\\nThe Red Hat Enterprise Linux Experience: Discover the value of Red Hat Enterprise Linux for your organization\\n\\n\\n\\nCase study\\n\\nRed Hat saves $5 million in IT support costs with AI augmentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRelated articles\\n\\n\\n\\n\\n\\n\\nWhy choose Red Hat Ansible Automation Platform as your AI foundation?\\n\\n\\nPredictive AI vs generative AI\\n\\n\\nWhat is agentic AI?\\n\\n\\nWhat is KVM?\\n\\n\\nWhat are Granite models? \\n\\n\\nLarge language models (LLMs) vs Small language models (SLMs)\\n\\n\\nRAG vs. fine-tuning\\n\\n\\nWhat is Podman Desktop?\\n\\n\\nUnderstanding AI in telecommunications with Red Hat\\n\\n\\nEdge solutions for real-time decision making\\n\\n\\nWhat is CentOS Stream?\\n\\n\\nWhat is CentOS?\\n\\n\\nWhat are CentOS replacements?\\n\\n\\nWhat are intelligent applications?\\n\\n\\nWhat is Podman?\\n\\n\\nWhat is retrieval-augmented generation?\\n\\n\\nWhat is Helm?\\n\\n\\nWhat is Argo CD?\\n\\n\\nWhat is an AI platform?\\n\\n\\nWhat is LLMops\\n\\n\\nWhat are predictive analytics\\n\\n\\nWhat is deep learning?\\n\\n\\nAI in banking\\n\\n\\nWhat is MicroShift?\\n\\n\\nAI infrastructure explained\\n\\n\\nUnderstanding AI/ML use cases\\n\\n\\nWhat is MLOps?\\n\\n\\nWhat are large language models?\\n\\n\\nWhat are foundation models for AI?\\n\\n\\nWhat is AIOps?\\n\\n\\nHow Kubernetes can help AI/ML\\n\\n\\nWhat is generative AI?\\n\\n\\nWhat is edge AI?\\n\\n\\nOpenJDK versus Oracle JDK\\n\\n\\nWhat is Cloud Foundry?\\n\\n\\nWhat is Kubeflow?\\n\\n\\nwhat is Buildah?\\n\\n\\nAccelerate MLOps with Red Hat OpenShift\\n\\n\\nUnderstanding Ansible, Terraform, Puppet, Chef, and Salt\\n\\n\\nAnsible vs. Chef: What you need to know\\n\\n\\nAnsible vs. Salt: What you need to know\\n\\n\\nWhat is Linux?\\n\\n\\nWhat\\'s the best Linux distro for you?\\n\\n\\nWhat is machine learning?\\n\\n\\nAnsible vs. Puppet: What you need to know\\n\\n\\nRed Hat OpenShift vs. OKD\\n\\n\\nWhat is AI in healthcare? \\n\\n\\nWhat is Apache Kafka?\\n\\n\\nSpring on Kubernetes with Red Hat OpenShift\\n\\n\\nWhy run Apache Kafka on Kubernetes?\\n\\n\\nAnsible vs. Terraform, clarified\\n\\n\\nAnsible vs. Red Hat Ansible Automation Platform\\n\\n\\nWhat is Skopeo?\\n\\n\\nUsing Helm with Red Hat OpenShift\\n\\n\\nWhat is Grafana?\\n\\n\\nWhat is open source software?\\n\\n\\nOpen source vs. proprietary software in vehicles\\n\\n\\nWhat is KubeLinter?\\n\\n\\nWhat is RKT?\\n\\n\\nWhat is Kogito?\\n\\n\\nWhat was CoreOS and CoreOS container Linux\\n\\n\\nWhat is Jaeger?\\n\\n\\nWhat is open source?\\n\\n\\nWhat is Clair?\\n\\n\\nWhat is Istio?\\n\\n\\nWhat is Knative?\\n\\n\\nWhat is etcd?\\n\\n\\nWhat is Docker?\\n\\n\\n\\n\\n\\n\\n\\nMore about this topic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLinkedInYouTubeFacebookXProductsRed Hat Enterprise LinuxRed Hat OpenShiftRed Hat Ansible Automation PlatformCloud servicesSee all productsToolsTraining and certificationMy accountCustomer supportDeveloper resourcesFind a partnerRed Hat Ecosystem CatalogRed Hat value calculatorDocumentationTry, buy, & sellProduct trial centerRed Hat MarketplaceRed Hat StoreBuy online (Japan)ConsoleCommunicateContact salesContact customer serviceContact trainingSocialAbout Red HatWe’re the world’s leading provider of enterprise open source solutions—including Linux, cloud, container, and Kubernetes. We deliver hardened solutions that make it easier for enterprises to work across platforms and environments, from the core datacenter to the network edge.Select a languageEnglish简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañolRed Hat legal and privacy linksAbout Red HatJobsEventsLocationsContact Red HatRed Hat BlogDiversity, equity, and inclusionCool Stuff StoreRed Hat Summit© 2025 Red Hat, Inc.Red Hat legal and privacy linksPrivacy statementTerms of useAll policies and guidelinesDigital accessibility\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader([\"https://www.ibm.com/topics/langchain\", \"https://www.redhat.com/en/topics/ai/what-is-instructlab\"])\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from WORD files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Docx2txtLoader` is utilized to convert Word documents into a document format suitable for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-06 20:41:13--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/94hiHUNLZdb0bLMkrCh79g/file-sample.docx\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1311881 (1.3M) [application/vnd.openxmlformats-officedocument.wordprocessingml.document]\n",
      "Saving to: ‘file-sample.docx’\n",
      "\n",
      "file-sample.docx    100%[===================>]   1.25M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-01-06 20:41:14 (47.3 MB/s) - ‘file-sample.docx’ saved [1311881/1311881]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/94hiHUNLZdb0bLMkrCh79g/file-sample.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Docx2txtLoader(\"file-sample.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'file-sample.docx'}, page_content='Demonstration of DOCX support in calibre\\n\\nThis document demonstrates the ability of the calibre DOCX Input plugin to convert the various typographic features in a Microsoft Word (2007 and newer) document. Convert this document to a modern ebook format, such as AZW3 for Kindles or EPUB for other ebook readers, to see it in action.\\n\\nThere is support for images, tables, lists, footnotes, endnotes, links, dropcaps and various types of text and paragraph level formatting.\\n\\nTo see the DOCX conversion in action, simply add this file to calibre using the “Add Books” button and then click “Convert”.  Set the output format in the top right corner of the conversion dialog to EPUB or AZW3 and click “OK”.\\n\\n\\n\\nText Formatting\\n\\nInline formatting\\n\\nHere, we demonstrate various types of inline text formatting and the use of embedded fonts.\\n\\nHere is some bold, italic, bold-italic, underlined and struck out  text. Then, we have a superscript and a subscript. Now we see some red, green and blue text. Some text with a yellow highlight. Some text in a box. Some text in inverse video.\\n\\nA paragraph with styled text: subtle emphasis  followed by strong text and intense emphasis. This paragraph uses document wide styles for styling rather than inline text properties as demonstrated in the previous paragraph — calibre can handle both with equal ease.\\n\\nFun with fonts\\n\\nThis document has embedded the Ubuntu font family. The body text is in the Ubuntu typeface, here is some text in the Ubuntu Mono typeface, notice how every letter has the same width, even i and m. Every embedded font will automatically be embedded in the output ebook during conversion. \\n\\nParagraph level formatting\\n\\nYou can do crazy things with paragraphs, if the urge strikes you. For instance this paragraph is right aligned and has a right border. It has also been given a light gray background.\\n\\nFor the lovers of poetry amongst you, paragraphs with hanging indents, like this often come in handy. You can use hanging indents to ensure that a line of poetry retains its individual identity as a line even when the screen is  too narrow to display it as a single line. Not only does this paragraph have a hanging indent, it is also has an extra top margin, setting it apart from the preceding paragraph.\\n\\nTables\\n\\nITEM\\n\\nNEEDED\\n\\nBooks\\n\\n1\\n\\nPens\\n\\n3\\n\\nPencils\\n\\n2\\n\\nHighlighter\\n\\n2 colors\\n\\nScissors\\n\\n1 pair\\n\\nTables in Word can vary from the extremely simple to the extremely complex. calibre tries to do its best when converting tables. While you may run into trouble with the occasional table, the vast majority of common cases should be converted very well, as demonstrated in this section. Note that for optimum results, when creating tables in Word, you should set their widths using percentages, rather than absolute units.  To the left of this paragraph is a floating two column table with a nice green border and header row.\\n\\nNow let’s look at a fancier table—one with alternating row colors and partial borders. This table is stretched out to take 100% of the available width.\\n\\nCity or Town\\n\\nPoint A\\n\\nPoint B\\n\\nPoint C\\n\\nPoint D\\n\\nPoint E\\n\\nPoint A\\n\\n—\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPoint B\\n\\n87\\n\\n—\\n\\n\\n\\n\\n\\n\\n\\nPoint C\\n\\n64\\n\\n56\\n\\n—\\n\\n\\n\\n\\n\\nPoint D\\n\\n37\\n\\n32\\n\\n91\\n\\n—\\n\\n\\n\\nPoint E\\n\\n93\\n\\n35\\n\\n54\\n\\n43\\n\\n—\\n\\n\\n\\nNext, we see a table with special formatting in various locations. Notice how the formatting for the header row and sub header rows is preserved.\\n\\nCollege\\n\\nNew students\\n\\nGraduating students\\n\\nChange\\n\\n\\n\\nUndergraduate\\n\\n\\n\\n\\n\\nCedar University\\n\\n110\\n\\n103\\n\\n+7\\n\\nOak Institute\\n\\n202\\n\\n210\\n\\n-8\\n\\n\\n\\nGraduate\\n\\n\\n\\n\\n\\nCedar University\\n\\n24\\n\\n20\\n\\n+4\\n\\nElm College\\n\\n43\\n\\n53\\n\\n-10\\n\\nTotal\\n\\n998\\n\\n908\\n\\n90\\n\\nSource: Fictitious data, for illustration purposes only\\n\\nNext, we have something a little more complex, a nested table, i.e. a table inside another table. Additionally, the inner table has some of its cells merged. The table is displayed horizontally centered.\\n\\nOne\\n\\nThree\\n\\nTwo\\n\\n\\n\\nFour\\n\\n\\n\\nTo the left is a table inside a table, with some cells merged.\\n\\n\\n\\nWe end with a fancy calendar, note how much of the original formatting is preserved. Note that this table will only display correctly on relatively wide screens. In general, very wide tables or tables whose cells have fixed width requirements don’t fare well in ebooks.\\n\\nDecember 2007\\n\\nSun\\n\\n\\n\\nMon\\n\\n\\n\\nTue\\n\\n\\n\\nWed\\n\\n\\n\\nThu\\n\\n\\n\\nFri\\n\\n\\n\\nSat\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\n\\n\\n\\n3\\n\\n\\n\\n4\\n\\n\\n\\n5\\n\\n\\n\\n6\\n\\n\\n\\n7\\n\\n\\n\\n8\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n9\\n\\n\\n\\n10\\n\\n\\n\\n11\\n\\n\\n\\n12\\n\\n\\n\\n13\\n\\n\\n\\n14\\n\\n\\n\\n15\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\n\\n17\\n\\n\\n\\n18\\n\\n\\n\\n19\\n\\n\\n\\n20\\n\\n\\n\\n21\\n\\n\\n\\n22\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n23\\n\\n\\n\\n24\\n\\n\\n\\n25\\n\\n\\n\\n26\\n\\n\\n\\n27\\n\\n\\n\\n28\\n\\n\\n\\n29\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n30\\n\\n\\n\\n31\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStructural Elements\\n\\nMiscellaneous structural elements you can add to your document, like footnotes, endnotes, dropcaps and the like. \\n\\nFootnotes & Endnotes\\n\\nFootnotes and endnotes are automatically recognized and both are converted to endnotes, with backlinks for maximum ease of use in ebook devices.\\n\\nDropcaps\\n\\nD\\n\\nrop caps are used to emphasize the leading paragraph at the start of a section. In Word it is possible to specify how many lines of text a drop-cap should use. Because of limitations in ebook technology, this is not possible when converting.  Instead, the converted drop cap will use font size and line height to simulate the effect as well as possible. While not as good as the original, the result is usually tolerable. This paragraph has a “D” dropcap set to occupy three lines of text with a font size of 58.5 pts. Depending on the screen width and capabilities of the device you view the book on, this dropcap can look anything from perfect to ugly.\\n\\nLinks\\n\\nTwo kinds of links are possible, those that refer to an external website and those that refer to locations inside the document itself. Both are supported by calibre. For example, here is a link pointing to the calibre download page. Then we have a link that points back to the section on paragraph level formatting in this document.\\n\\nTable of Contents\\n\\nThere are two approaches that calibre takes when generating a Table of Contents. The first is if the Word document has a Table of Contents itself. Provided that the Table of Contents uses hyperlinks, calibre will automatically use it. The levels of the Table of Contents are identified by their left indent, so if you want the ebook to have a multi-level Table of Contents, make sure you create a properly indented Table of Contents in Word.\\n\\nIf no Table of Contents is found in the document, then a table of contents is automatically generated from the headings in the document. A heading is identified as something that has the Heading 1 or Heading 2, etc. style applied to it. These headings are turned into a Table of Contents with Heading 1 being the topmost level, Heading 2 the second level and so on.\\n\\n You can see the Table of Contents created by calibre by clicking the Table of Contents button in whatever viewer you are using to view the converted ebook. \\n\\n\\tDemonstration of DOCX support in calibre\\t1\\n\\n\\tText Formatting\\t2\\n\\n\\tInline formatting\\t2\\n\\n\\tFun with fonts\\t2\\n\\n\\tParagraph level formatting\\t2\\n\\n\\tTables\\t3\\n\\n\\tStructural Elements\\t5\\n\\n\\tFootnotes & Endnotes\\t5\\n\\n\\tDropcaps\\t5\\n\\n\\tLinks\\t5\\n\\n\\tTable of Contents\\t5\\n\\n\\tImages\\t7\\n\\n\\tLists\\t8\\n\\n\\tBulleted List\\t8\\n\\n\\tNumbered List\\t8\\n\\n\\tMulti-level Lists\\t8\\n\\n\\tContinued Lists\\t8\\n\\n\\n\\n\\n\\nImages\\n\\nImages can be of three main types. Inline images are images that are part of the normal text flow, like this image of a green dot . Inline images do not cause breaks in the text and are usually small in size. The next category of image is a floating image, one that “floats “ on the page and is surrounded by text. Word supports more types of floating images than are possible with current ebook technology, so the conversion maps floating images to simple left and right floats, as you can see with the left and right arrow images on the sides of this paragraph.\\n\\nThe final type of image is a “block” image, one that becomes a paragraph on its own and has no text on either side. Below is a centered green dot.\\n\\nCentered images like this are useful for large pictures that should be a focus of attention. \\n\\nGenerally, it is not possible to translate the exact positioning of images from a Word document to an ebook. That is because in Word, image positioning is specified in absolute units from the page boundaries.  There is no analogous technology in ebooks, so the conversion will usually end up placing the image either centered or floating close to the point in the text where it was inserted, not necessarily where it appears on the page in Word.\\n\\nLists\\n\\nAll types of lists are supported by the conversion, with the exception of lists that use fancy bullets, these get converted to regular bullets.\\n\\nBulleted List\\n\\nOne\\n\\nTwo\\n\\nNumbered List\\n\\nOne, with a very long line to demonstrate that the hanging indent for the list is working correctly\\n\\nTwo\\n\\nMulti-level Lists\\n\\nOne\\n\\nTwo\\n\\nThree\\n\\nFour with a very long line to demonstrate that the hanging indent for the list is working correctly.\\n\\nFive\\n\\nSix\\n\\nA Multi-level list with bullets:\\n\\nOne\\n\\nTwo\\n\\nThis bullet uses an image as the bullet item\\n\\nFour\\n\\nFive\\n\\nContinued Lists\\n\\nOne\\n\\nTwo\\n\\nAn interruption in our regularly scheduled listing, for this essential and very relevant public service announcement.\\n\\nWe now resume our normal programming\\n\\nFour')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Unstructured Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content=\"1. Code of Conduct\\n\\nOur Code of Conduct establishes the core values and ethical standards that all members of our organization must adhere to. We are committed to fostering a workplace characterized by integrity, respect, and accountability.\\n\\nIntegrity: We commit to the highest ethical standards by being honest and transparent in all our dealings, whether with colleagues, clients, or the community. We protect sensitive information and avoid conflicts of interest.\\n\\nRespect: We value diversity and every individual's contribution. Discrimination, harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where differences are respected, and everyone is treated with dignity.\\n\\nAccountability: We are responsible for our actions and decisions, complying with all relevant laws and regulations. We aim for continuous improvement and report any breaches of this code, supporting investigations into such matters.\\n\\nSafety: We prioritize the safety of our employees, clients, and the community. We encourage a culture of safety, including reporting any unsafe practices or conditions.\\n\\nEnvironmental Responsibility: We strive to reduce our environmental impact and promote sustainable practices.\\n\\nThis Code of Conduct is the cornerstone of our organizational culture. We expect every employee to uphold these principles and act as role models, ensuring our reputation for ethical conduct, integrity, and social responsibility.\\n\\n2. Recruitment Policy\\n\\nOur Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified and diverse candidates into our organization. The success of our company depends on the talent, skills, and commitment of our employees.\\n\\nEqual Opportunity: We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively support diversity and inclusion.\\n\\nTransparency: We maintain a transparent recruitment process. Job vacancies are advertised both internally and externally when appropriate. Job descriptions and requirements are clear and accurately reflect the role.\\n\\nSelection Criteria: We base our selection on qualifications, experience, and skills relevant to the role. Our interviews and assessments are objective, and decisions are made impartially.\\n\\nData Privacy: We are dedicated to protecting candidates' personal information and comply with all applicable data protection laws.\\n\\nFeedback: Candidates receive timely and constructive feedback on their applications and interview performance.\\n\\nOnboarding: New hires receive thorough onboarding to help them integrate effectively, including an overview of our culture, policies, and expectations.\\n\\nEmployee Referrals: We welcome employee referrals as they help build a strong and engaged team.\\n\\nThis policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\\n\\n3. Internet and Email Policy\\n\\nOur Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\\n\\nAcceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\\n\\nSecurity: Protect your login credentials and avoid sharing passwords. Be cautious with email attachments and links from unknown sources, and promptly report any unusual online activity or potential security threats.\\n\\nConfidentiality: Use email for confidential information, trade secrets, and sensitive customer data only with encryption. Be careful when discussing company matters on public platforms or social media.\\n\\nHarassment and Inappropriate Content: Internet and email must not be used for harassment, discrimination, or the distribution of offensive content. Always communicate respectfully and sensitively online.\\n\\nCompliance: Adhere to all relevant laws and regulations concerning internet and email use, including copyright and data protection laws.\\n\\nMonitoring: The company reserves the right to monitor internet and email usage for security and compliance purposes.\\n\\nConsequences: Violations of this policy may lead to disciplinary action, including potential termination.\\n\\nThis policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\\n\\n4. Mobile Phone Policy\\n\\nOur Mobile Phone Policy defines standards for responsible use of mobile devices within the organization to ensure alignment with company values and legal requirements.\\n\\nAcceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is allowed if it does not disrupt work responsibilities.\\n\\nSecurity: Secure your mobile device and credentials. Be cautious with app downloads and links from unknown sources, and report any security issues promptly.\\n\\nConfidentiality: Avoid sharing sensitive company information via unsecured messaging apps or emails. Exercise caution when discussing company matters in public.\\n\\nCost Management: Personal use of mobile phones should be separate from company accounts, and any personal charges on company-issued phones must be reimbursed.\\n\\nCompliance: Comply with all relevant laws and regulations concerning mobile phone usage, including data protection and privacy laws.\\n\\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\\n\\nConsequences: Non-compliance with this policy may result in disciplinary actions, including potential loss of mobile phone privileges.\\n\\nThis policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.\")]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredFileLoader(\"new-Policies.txt\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'markdown-sample.md'}, page_content='An h1 header\\n\\nParagraphs are separated by a blank line.\\n\\n2nd paragraph. Italic, bold, and monospace. Itemized lists\\nlook like:\\n\\nthis one\\n\\nthat one\\n\\nthe other one\\n\\nNote that --- not considering the asterisk --- the actual text\\ncontent starts at 4-columns in.\\n\\nBlock quotes are\\nwritten like so.\\n\\nThey can span multiple paragraphs,\\nif you like.\\n\\nUse 3 dashes for an em-dash. Use 2 dashes for ranges (ex., \"it\\'s all\\nin chapters 12--14\"). Three dots ... will be converted to an ellipsis.\\nUnicode is supported. ☺\\n\\nAn h2 header\\n\\nHere\\'s a numbered list:\\n\\nfirst item\\n\\nsecond item\\n\\nthird item\\n\\nNote again how the actual text starts at 4 columns in (4 characters\\nfrom the left side). Here\\'s a code sample:\\n\\nAs you probably guessed, indented 4 spaces. By the way, instead of\\nindenting the block, you can use delimited blocks, if you like:\\n\\n~~~\\ndefine foobar() {\\n    print \"Welcome to flavor country!\";\\n}\\n~~~\\n\\n(which makes copying & pasting easier). You can optionally mark the\\ndelimited block for Pandoc to syntax highlight it:\\n\\n~~~python\\nimport time\\n\\nQuick, count to ten!\\n\\nfor i in range(10):\\n    # (but not too quick)\\n    time.sleep(0.5)\\n    print i\\n~~~\\n\\nAn h3 header\\n\\nNow a nested list:\\n\\nFirst, get these ingredients:\\n\\ncarrots\\ncelery\\nlentils\\n\\nBoil some water.\\n\\nDump everything in the pot and follow\\n    this algorithm:\\nfind wooden spoon\\nuncover pot\\nstir\\ncover pot\\nbalance wooden spoon precariously on pot handle\\nwait 10 minutes\\ngoto first step (or shut off burner when done)\\n\\nDo not bump wooden spoon or it will fall.\\n\\nNotice again how text always lines up on 4-space indents (including\\nthat last line which continues item 3 above).\\n\\nHere\\'s a link to a website, to a local\\ndoc, and to a section heading in the current\\ndoc. Here\\'s a footnote [^1].\\n\\n[^1]: Footnote text goes here.\\n\\nTables can look like this:\\n\\nsize  material      color\\n\\n9     leather       brown\\n10    hemp canvas   natural\\n11    glass         transparent\\n\\nTable: Shoes, their sizes, and what they\\'re made of\\n\\n(The above is the caption for the table.) Pandoc also supports\\nmulti-line tables:\\n\\nkeyword   text\\n\\nred       Sunsets, apples, and\\n          other red or reddish\\n          things.\\n\\ngreen     Leaves, grass, frogs\\n          and other things it\\'s\\n          not easy being.\\n\\nA horizontal rule follows.\\n\\nHere\\'s a definition list:\\n\\napples\\n  : Good for making applesauce.\\noranges\\n  : Citrus!\\ntomatoes\\n  : There\\'s no \"e\" in tomatoe.\\n\\nAgain, text is indented 4 spaces. (Put a blank line between each\\nterm/definition pair to spread things out more.)\\n\\nHere\\'s a \"line block\":\\n\\n| Line one\\n|   Line too\\n| Line tree\\n\\nand images can be specified like so:\\n\\nInline math equations go in like so: $\\\\omega = d\\\\phi / dt$. Display\\nmath should get its own line and be put in in double-dollarsigns:\\n\\n$$I = \\\\int \\\\rho R^{2} dV$$\\n\\nAnd note that you can backslash-escape any punctuation characters\\nwhich you wish to be displayed literally, ex.: `foo`, *bar*, etc.')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredFileLoader(\"markdown-sample.md\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple files with different formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even load a list of files with different formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"markdown-sample.md\", \"new-Policies.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': ['markdown-sample.md', 'new-Policies.txt']}, page_content='An h1 header\\n\\nParagraphs are separated by a blank line.\\n\\n2nd paragraph. Italic, bold, and monospace. Itemized lists\\nlook like:\\n\\nthis one\\n\\nthat one\\n\\nthe other one\\n\\nNote that --- not considering the asterisk --- the actual text\\ncontent starts at 4-columns in.\\n\\nBlock quotes are\\nwritten like so.\\n\\nThey can span multiple paragraphs,\\nif you like.\\n\\nUse 3 dashes for an em-dash. Use 2 dashes for ranges (ex., \"it\\'s all\\nin chapters 12--14\"). Three dots ... will be converted to an ellipsis.\\nUnicode is supported. ☺\\n\\nAn h2 header\\n\\nHere\\'s a numbered list:\\n\\nfirst item\\n\\nsecond item\\n\\nthird item\\n\\nNote again how the actual text starts at 4 columns in (4 characters\\nfrom the left side). Here\\'s a code sample:\\n\\nAs you probably guessed, indented 4 spaces. By the way, instead of\\nindenting the block, you can use delimited blocks, if you like:\\n\\n~~~\\ndefine foobar() {\\n    print \"Welcome to flavor country!\";\\n}\\n~~~\\n\\n(which makes copying & pasting easier). You can optionally mark the\\ndelimited block for Pandoc to syntax highlight it:\\n\\n~~~python\\nimport time\\n\\nQuick, count to ten!\\n\\nfor i in range(10):\\n    # (but not too quick)\\n    time.sleep(0.5)\\n    print i\\n~~~\\n\\nAn h3 header\\n\\nNow a nested list:\\n\\nFirst, get these ingredients:\\n\\ncarrots\\ncelery\\nlentils\\n\\nBoil some water.\\n\\nDump everything in the pot and follow\\n    this algorithm:\\nfind wooden spoon\\nuncover pot\\nstir\\ncover pot\\nbalance wooden spoon precariously on pot handle\\nwait 10 minutes\\ngoto first step (or shut off burner when done)\\n\\nDo not bump wooden spoon or it will fall.\\n\\nNotice again how text always lines up on 4-space indents (including\\nthat last line which continues item 3 above).\\n\\nHere\\'s a link to a website, to a local\\ndoc, and to a section heading in the current\\ndoc. Here\\'s a footnote [^1].\\n\\n[^1]: Footnote text goes here.\\n\\nTables can look like this:\\n\\nsize  material      color\\n\\n9     leather       brown\\n10    hemp canvas   natural\\n11    glass         transparent\\n\\nTable: Shoes, their sizes, and what they\\'re made of\\n\\n(The above is the caption for the table.) Pandoc also supports\\nmulti-line tables:\\n\\nkeyword   text\\n\\nred       Sunsets, apples, and\\n          other red or reddish\\n          things.\\n\\ngreen     Leaves, grass, frogs\\n          and other things it\\'s\\n          not easy being.\\n\\nA horizontal rule follows.\\n\\nHere\\'s a definition list:\\n\\napples\\n  : Good for making applesauce.\\noranges\\n  : Citrus!\\ntomatoes\\n  : There\\'s no \"e\" in tomatoe.\\n\\nAgain, text is indented 4 spaces. (Put a blank line between each\\nterm/definition pair to spread things out more.)\\n\\nHere\\'s a \"line block\":\\n\\n| Line one\\n|   Line too\\n| Line tree\\n\\nand images can be specified like so:\\n\\nInline math equations go in like so: $\\\\omega = d\\\\phi / dt$. Display\\nmath should get its own line and be put in in double-dollarsigns:\\n\\n$$I = \\\\int \\\\rho R^{2} dV$$\\n\\nAnd note that you can backslash-escape any punctuation characters\\nwhich you wish to be displayed literally, ex.: `foo`, *bar*, etc.\\n\\n1. Code of Conduct\\n\\nOur Code of Conduct establishes the core values and ethical standards that all members of our organization must adhere to. We are committed to fostering a workplace characterized by integrity, respect, and accountability.\\n\\nIntegrity: We commit to the highest ethical standards by being honest and transparent in all our dealings, whether with colleagues, clients, or the community. We protect sensitive information and avoid conflicts of interest.\\n\\nRespect: We value diversity and every individual\\'s contribution. Discrimination, harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where differences are respected, and everyone is treated with dignity.\\n\\nAccountability: We are responsible for our actions and decisions, complying with all relevant laws and regulations. We aim for continuous improvement and report any breaches of this code, supporting investigations into such matters.\\n\\nSafety: We prioritize the safety of our employees, clients, and the community. We encourage a culture of safety, including reporting any unsafe practices or conditions.\\n\\nEnvironmental Responsibility: We strive to reduce our environmental impact and promote sustainable practices.\\n\\nThis Code of Conduct is the cornerstone of our organizational culture. We expect every employee to uphold these principles and act as role models, ensuring our reputation for ethical conduct, integrity, and social responsibility.\\n\\n2. Recruitment Policy\\n\\nOur Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified and diverse candidates into our organization. The success of our company depends on the talent, skills, and commitment of our employees.\\n\\nEqual Opportunity: We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively support diversity and inclusion.\\n\\nTransparency: We maintain a transparent recruitment process. Job vacancies are advertised both internally and externally when appropriate. Job descriptions and requirements are clear and accurately reflect the role.\\n\\nSelection Criteria: We base our selection on qualifications, experience, and skills relevant to the role. Our interviews and assessments are objective, and decisions are made impartially.\\n\\nData Privacy: We are dedicated to protecting candidates\\' personal information and comply with all applicable data protection laws.\\n\\nFeedback: Candidates receive timely and constructive feedback on their applications and interview performance.\\n\\nOnboarding: New hires receive thorough onboarding to help them integrate effectively, including an overview of our culture, policies, and expectations.\\n\\nEmployee Referrals: We welcome employee referrals as they help build a strong and engaged team.\\n\\nThis policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\\n\\n3. Internet and Email Policy\\n\\nOur Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\\n\\nAcceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\\n\\nSecurity: Protect your login credentials and avoid sharing passwords. Be cautious with email attachments and links from unknown sources, and promptly report any unusual online activity or potential security threats.\\n\\nConfidentiality: Use email for confidential information, trade secrets, and sensitive customer data only with encryption. Be careful when discussing company matters on public platforms or social media.\\n\\nHarassment and Inappropriate Content: Internet and email must not be used for harassment, discrimination, or the distribution of offensive content. Always communicate respectfully and sensitively online.\\n\\nCompliance: Adhere to all relevant laws and regulations concerning internet and email use, including copyright and data protection laws.\\n\\nMonitoring: The company reserves the right to monitor internet and email usage for security and compliance purposes.\\n\\nConsequences: Violations of this policy may lead to disciplinary action, including potential termination.\\n\\nThis policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\\n\\n4. Mobile Phone Policy\\n\\nOur Mobile Phone Policy defines standards for responsible use of mobile devices within the organization to ensure alignment with company values and legal requirements.\\n\\nAcceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is allowed if it does not disrupt work responsibilities.\\n\\nSecurity: Secure your mobile device and credentials. Be cautious with app downloads and links from unknown sources, and report any security issues promptly.\\n\\nConfidentiality: Avoid sharing sensitive company information via unsecured messaging apps or emails. Exercise caution when discussing company matters in public.\\n\\nCost Management: Personal use of mobile phones should be separate from company accounts, and any personal charges on company-issued phones must be reimbursed.\\n\\nCompliance: Comply with all relevant laws and regulations concerning mobile phone usage, including data protection and privacy laws.\\n\\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\\n\\nConsequences: Non-compliance with this policy may result in disciplinary actions, including potential loss of mobile phone privileges.\\n\\nThis policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other PDF loaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdfium2\n",
      "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdfium2\n",
      "Successfully installed pypdfium2-4.30.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdfium2\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "loader = PyPDFium2Loader(pdf_url)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0}, page_content='LAB: LARGE-SCALE ALIGNMENT FOR CHATBOTS\\r\\nMIT-IBM Watson AI Lab and IBM Research\\r\\nShivchander Sudalairaj∗\\r\\nAbhishek Bhandwaldar∗\\r\\nAldo Pareja∗\\r\\nKai Xu\\r\\nDavid D. Cox\\r\\nAkash Srivastava∗,†\\r\\n*Equal Contribution, †Corresponding Author\\r\\nABSTRACT\\r\\nThis work introduces LAB (Large-scale Alignment for chatBots), a novel method\\x02ology designed to overcome the scalability challenges in the instruction-tuning\\r\\nphase of large language model (LLM) training. Leveraging a taxonomy-guided\\r\\nsynthetic data generation process and a multi-phase tuning framework, LAB sig\\x02nificantly reduces reliance on expensive human annotations and proprietary mod\\x02els like GPT-4. We demonstrate that LAB-trained models can achieve compet\\x02itive performance across several benchmarks compared to models trained with\\r\\ntraditional human-annotated or GPT-4 generated synthetic data. Thus offering a\\r\\nscalable, cost-effective solution for enhancing LLM capabilities and instruction\\x02following behaviors without the drawbacks of catastrophic forgetting, marking a\\r\\nstep forward in the efficient training of LLMs for a wide range of applications.\\r\\n1 INTRODUCTION\\r\\nLarge language models (LLMs) have achieved remarkable levels of success in various natural lan\\x02guage processing (NLP) applications, including question-answering , entity extraction , and sum\\x02marization . This has been made possible, in large part, by the introduction of the transformer\\r\\narchitecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal\\x02ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\\r\\nself-supervised pre-training phase, followed by supervised alignment tuning phases.\\r\\nThe majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\\r\\nmodel is trained in an auto-regressive manner to predict the next token in the target language using\\r\\ntrillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\\r\\ntime. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer\\x02ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\\r\\nlearning, where the model is trained directly on tasks of interest. In this stage, the model is given a\\r\\ntask description in the form of an natural language instuction (e.g. Summarize the following news\\r\\narticle in 2 lines: {News article}) and the model is trained to maximize the likelihood of the pro\\x02vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\\r\\nas RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\\r\\nresponse from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\\r\\nIn comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\\r\\nfraction of the overall training procedure, both in terms of the data used as well as the compute\\r\\ninfrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\\r\\nwere trained with just tens of thousands of high quality human-generated instruction/response data\\r\\npairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\\r\\ncompared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\\r\\ntraining perspective, this imbalance in the scale across the phases is unconventional—typically one\\r\\nwould expect a model to perform best when it has been trained directly on the desired tasks, using as\\r\\nmuch data as possible. The deviation from the tradtional LLM approach relies on the idea that pre\\x021\\r\\narXiv:2403.01081v3 [cs.CL] 29 Apr 2024\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 1}, page_content='training captures enough of the distribution of language and knowledge, such that a small amount of\\r\\nsupervised training can “unlock” or shape latent abilities related to the ultimate desired instruction\\x02following behavior of the model. However, unlike the unstructured data that is abundantly available\\r\\nin the public domain, high-quality, human-generated task-specific instruction data is costly to pro\\x02cure, even via crowd-sourcing, and human-generated instruction data is typically closely guarded\\r\\nby model builders, even for ostensibly “open” model-building efforts. In this work, we address\\r\\nthe challenges associated with scaling of the alignment-tuning phase and propose a new method\\r\\ncalled LAB: Large-scale Alignment for chatBots. The LAB method consists of two components:\\r\\n(i) a taxonomy-guided synthetic data generation method and quality assurance process that yields a\\r\\nhighly diverse and high-quality instruction dataset, without resorting to the use of proprietary LLMs\\r\\nlike GPT-4 or substantial human curation, and (ii) a novel multi-phase training framework and un\\x02conventional tuning regime that allows for adding new knowledge and instruction-following abilities\\r\\ninto pre-trained LLMs without suffering from catastrophic forgetting. Our findings show that LAB\\x02trained models can perform competitively with proprietary and open-source models that use human\\r\\nannotations and/or synthetic data generated using GPT-4 on a number of benchmarks.\\r\\n2 RELATED WORK\\r\\nExisting methods for instruction tuning typically either rely on humans for generating high-quality\\r\\ndatasets, or use synthetic data generation using a large teacher model. OpenAI (Ouyang et al.,\\r\\n2022) arguably set the standard for model alignment from human data, employing human annota\\x02tors to gather data for supervised fine tuning (SFT) and reinforcement learning with human feed\\x02back (RLHF) training. Collecting human-generated data for these steps is complex undertaking; the\\r\\nselection of annotators requires a rigorous multi-stage screening process aimed at achieving high\\r\\ninter-annotator agreement, and collecting even modest amounts data (by LLM standards) requires\\r\\nthe coordination of large groups of annotators. The creators of the LLaMA 2 model series (Touvron\\r\\net al., 2023) followed a similar recipe, collecting tens of thousands of human-generated instruction\\r\\nsamples, and approximately 1 million human-annotated binary comparisons for reward modeling.\\r\\nNot only are such approaches expensive and time consuming, but they can also potentially limit\\r\\nagility in exploring the space of instructions and capabilities the model is trained to perform. Alter\\x02natives to this approach, such as transforming existing human datasets into instructions via templat\\x02ing (Wei et al.) can be more cost effective, but face limitations in the naturalness and length of the\\r\\nresponses used for training.\\r\\nMore recently, training with synthetic data generated from LLMs has emerged as an alternative to\\r\\npurely human-data-based approaches. Wang et al. (2023) introduced Self-Instruct, which leverages\\r\\na small number of handwritten human seed instructions as input to bootstrapping process to generate\\r\\na large number of samples using an LLM’s own generation abilities. Taori et al. (2023) built upon\\r\\nSelf-Instruct, using a larger teacher model to generate synthetic data to train a smaller student model,\\r\\nand incorporating principles in the generation prompt to promote diversity in the generated instruc\\x02tion data. Xu et al. (2023) introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes\\r\\niteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee\\r\\net al. (2023), Mitra et al. (2023) present a synthetic data generation approach to enhance task di\\x02versity and scalability, alongside a progressive training framework aimed at improving the model’s\\r\\nreasoning ability and response style to match teacher models. This is achieved by generating rich\\r\\nreasoning signals in the generated answer and progressively training on datasets of varying difficulty\\r\\nin incremental phases.\\r\\nSimilar to LAB, concurrent work, GLAN (Li et al., 2024), employs a semi-automatic approach to\\r\\nsynthetic data generation that uses a human-curated taxonomy to generate instruction tuning data\\r\\nfrom a teacher model. However, as explained in section 3.2.2, unlike LAB, GLAN cannot be used\\r\\nto generate synthetic data from domains that are not captured in the teacher model’s support. As\\r\\nsuch, while LAB uses the open-source Mixtral model as the teacher, like many other synthetic\\r\\ndata generation approaches, GLAN has to rely on a large proprietary model (GPT-4). This poses\\r\\ncomplicated questions about the usability of generated data (especially for commercial purposes)\\r\\nsince the terms of use of proprietary models typically forbid using the model to improve other\\r\\nmodels.\\r\\n2\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 2}, page_content='taxonomy root\\r\\nknowledge\\r\\n. . . textbook\\r\\nfinance\\r\\nexample 1 example 2 example 3\\r\\n. . .\\r\\nfoundational\\r\\nskills\\r\\n. . . mathematics\\r\\narithmetic\\r\\naddition\\r\\nmath instruct data\\r\\n. . .\\r\\ncompositional\\r\\nskills\\r\\n. . . writing\\r\\nemail\\r\\nearnings report\\r\\nexample 1 example 2 example 3\\r\\n. . .\\r\\ndocument\\r\\nSynthetic Data Generator 1 Synthetic Data Generator 2\\r\\nsynthetic “finance” data 0.1–2k synthetic “email” data 0.1–2k\\r\\npre-trained LLM Phased training\\r\\nsection 3.1\\r\\nsection 3.2\\r\\nsection 3.3\\r\\nFigure 1: Overview of the LAB alignment method. Starting from the taxonomy root, data are curated\\r\\nin each top-level groups and examples in the leaf nodes are used by the synthetic data generators to\\r\\ngenerate orders of magnitude data for the phased-training step for instruct-tuning.\\r\\n3 METHODOLOGY\\r\\nLAB consists of two components: (i) a taxonomy to enable data curation (section 3.1) as well as,\\r\\nguide the synthetic data generator (section 3.2) and (ii) a multi-phased instruction-tuning method\\r\\nwith replay buffers to enable large-scale alignment-tuning. (section 3.3). (i) serves the purpose of\\r\\nensuring high diversity and quality in the synthetically generated instruction-tuning dataset while\\r\\n(ii) ensures training stability and prevents catastrophic forgetting. Figure 1 provides an overview of\\r\\nthe end-to-end pipeline of applying the LAB method to align a pre-trained LLM.\\r\\n3.1 TAXONOMY\\r\\nTo enable the data curator or the model designer to organize the instruction-tuning training data, we\\r\\ndefine a taxonomy that hierarchically classifies the data samples into smaller task groups. At a high\\r\\nlevel, the taxonomy has three main branches: knowledge, foundational skills, and compositional\\r\\nskills. Each of these branches is further split into more granular levels where the tasks are defined\\r\\nin the leaf nodes and exemplified by providing manually written instruction-response pairs. This\\r\\nallows for easily identifying missing tasks in the target LLM and other tasks of interest and adding\\r\\nthem to the training data pool. New tasks are added to the taxonomy by creating a leaf node under\\r\\nthe appropriate branch and attaching 1–3 examples.\\r\\nKnowledge The knowledge branch in the taxonomy is first divided based on document types like\\r\\ntextbooks, technical manuals, etc., which are further divided into various domains like finance, statis\\x02tics, etc.; see the sub-tree for knowledge in Figure 1 as an example. Each domain has a collection of\\r\\ndocuments and a sample set of domain-specific questions and answers. This organization allows for\\r\\nbetter control over the licensing of text documents. As described in the next section, only the doc\\x02uments with permissible licenses are selected for synthetic data generation, excluding knowledge\\r\\nsources that lack proper licensing, reinforcing the integrity of our knowledge-generation processes.\\r\\n3\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 3}, page_content='Foundational skills We identify mathematics, coding, linguistic ability and reasoning as founda\\x02tional skills that the model requires to prime itself for better knowledge acquisition and build further\\r\\ncomplex and compositional skills. To teach the model foundational skills, we employ publicly avail\\x02able datasets (Longpre et al., 2023; Xiang Yue, 2023; Yin et al., 2018; Trivedi et al., 2022); see the\\r\\nsub-tree for foundational skills in Figure 1 for an example.\\r\\nCompositional skills Compositional skills refer to the tasks that require a combination of knowl\\x02edge and foundational skills, synergistically, to answer complex queries from users. For instance, the\\r\\nmodel’s ability to write a company-wide email sharing insights about the company’s performance\\r\\nlast quarter and guidance for the upcoming year would require the model to understand the financial\\r\\naspects of revenue, profit and loss, the skills of doing basic arithmetic and also have the skills to\\r\\ncompose a formal email.\\r\\n3.2 TAXONOMY-DRIVEN SYNTHETIC DATA GENERATOR\\r\\nThe small number of manually curated data samples, embedded in the leaf nodes of the taxonomy,\\r\\ncan be directly used for instruction tuning of the chatbot, however, the model may still perform\\r\\npoorly. Prior work (Li et al., 2023) has shown that typically, a large amount of high-quality instruc\\x02tion data is required for improving instruction following performance of LLMs. It is possible to\\r\\nleverage existing SDGs like Wang et al. (2023); Taori et al. (2023) to use the embedded examples\\r\\nand generate a lot more instruction data synthetically using teacher LLMs. But, such distillation\\x02based SDGs tend to over-sample from the dominant modes of the teacher model and thus lack in\\r\\ndiversity and quality of the generated data Gudibande et al. (2023). We argue that this limitation is\\r\\nattributed to the random selection of examples from the pool of seed samples: with random selec\\x02tion, the examples used to prompt the teacher model at each time are an “average” of the seed pool\\r\\ni.e. they do not focus on any specific task. This lack of focus tends to encourage the teacher model to\\r\\ngenerate more synthetic data from its dominant modes and ignore the long tail of interesting tasks.\\r\\nTo address this issue, we replace the random sampling in existing SDGs with a taxonomy-driven\\r\\napproach to guide the sampling of synthetic data, enabling targeted coverage of the support of the\\r\\nteacher model distribution around the individual leaf nodes of the taxonomy. Figure 2 illustrate the\\r\\nhigh-level idea behind this change. Figure 2a shows the issue of randomly sampling in the input\\r\\ntask domain\\r\\ninput distribution\\r\\nseed examples\\r\\nself-instruct\\r\\ntaxonomy-driven\\r\\n(a) Input distributions\\r\\ntask domain\\r\\noutput distribution\\r\\nteacher model\\r\\nw/ self-instruct\\r\\nw/ taxonomy-driven\\r\\n(b) Output distributions\\r\\nFigure 2: Intuition of how taxonomy-driven sampling produces diverse set of synthetic data and\\r\\nhence improve the data used to train student model across the task domain. Figure 2a shows how\\r\\ntaxonomy-driven sampling leads to an input distribution with wide support and distinct modes while\\r\\nself-instruct gives an smooth input distribution. Figure 2b shows the consequence using inputs in\\r\\ngenerating synthetic data: teacher model will focus its own dominant modes if the input is smooth\\r\\nbut focus on each task better if the inputs are also concentrated on each task.\\r\\nspace of the teacher model (i.e. prompts). Given a set of seed examples (red), randomly sampling\\r\\nwith more than one example gives an approximation to the average of the seed pool, leading to a\\r\\nsmoothed distribution, e.g. self-instruct distribution (blue). With the taxonomy-driven sampling,\\r\\nsince only the examples within each of the leaf nodes are used when sampling for the corresponding\\r\\ntasks, each of the tasks are guaranteed to be well represented in the prompts (purple). Second, when\\r\\n4\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 4}, page_content='You are asked to come up with a set of {num samples} diverse questions on {task}.\\r\\nPlease follow these guiding principles when generating responses:\\r\\n* Use proper grammar and punctuation.\\r\\n* Always generate safe and respectful content. Do not generate content that is harmful, abusive, or\\r\\noffensive.\\r\\n* Always generate content that is factually accurate and relevant to the prompt.\\r\\n* The questions should be clear and human-like.\\r\\n* The questions should be diverse and cover a wide range of topics.\\r\\n* The questions should not be template-based or generic, it should be very diverse.\\r\\n* Simply return the questions, do not return any answers or explanations.\\r\\n* Strictly adhere to the prompt and generate responses in the same style and format as the example.\\r\\nTo better assist you with this task, here is an example:\\r\\n### Question:\\r\\n1. {icl question}\\r\\nNow generate {num samples} such questions, remember to follow the principles mentioned above\\r\\nand use the same format as the examples. Remember to use the same style and format as the example\\r\\nabove. Return your responses in the format of [### Question [question number]: [question]]\\r\\nFigure 3: Instruction Generator prompt template\\r\\nit comes to the output space, for a given teacher model (red), prompting it with random examples\\r\\n(i.e. smoothened input distribution) tends to make it sampling from its own dominant mode (blue)\\r\\nwhile prompting it with focused examples in each leaf node (i.e. input distribution with distinct\\r\\nmodes), the teacher model is guaranteed to generate synthetic data for each of the tasks (purple).\\r\\nWith the above insight, we now introduce two new synthetic data generation (SDG) methods in LAB\\r\\nthat leverage the taxonomy to guide the data generation process. The first one is targeted for skills\\r\\ngeneration and uses the handful of task examples in the leaf nodes to generate a lot more using the\\r\\nopen-source Mixtral-7x8B model. The second one is targeted at knowledge generation. While it\\r\\nstill uses the Mixtral-7x8B model, unlike prior works, it does not rely on the knowledge stored in\\r\\nthe teacher model.\\r\\n3.2.1 SKILL GENERATION\\r\\nSkills-SDG uses four prompt templates, one for each of the four, below-mentioned, stages of data\\r\\ngeneration. Each template has its own set of principles and instructions that control the role of the\\r\\nteacher model (generator vs evaluator) and guide the generation/evaluation process.\\r\\n1. Instruction generation: In the first stage, the teacher model acts as a question generator,\\r\\nusing a specialized prompt (see Figure 3 for an example) to leverage its knowledge and\\r\\ncreate diverse questions. By iterating through each leaf node of a taxonomy, the teacher\\r\\ngenerates queries that adhere to specific principles and thoroughly explore the targeted\\r\\ndomain, enhancing the comprehensiveness of the generated content.\\r\\n2. Evaluating synthetic instruction: In this stage, the teacher model assumes the role of\\r\\nan instruction evaluator, the teacher model uses targeted prompts to filter out questions\\r\\nthat don’t meet predefined principles, including relevance to the domain, potential harm,\\r\\nor questions beyond a language model’s answering capabilities. This ensures that only\\r\\nhigh-quality, contextually appropriate questions move forward in the process.\\r\\n3. Generating responses: The teacher model, functioning as a response generator in this\\r\\nstage, adopts dual personas for precision and creativity, guided by distinct prompts. This\\r\\ntailored approach helps to generate both, creative responses for domains like writing and\\r\\nrole-play, and precise answers for STEM and data extraction, aligning the response style to\\r\\nhuman expectations through principles and seed examples in the leaf nodes.\\r\\n5\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 5}, page_content='Please act as an impartial judge and evaluate the quality of the answer provided by an AI assistant\\r\\nto the questions displayed below. Evaluate whether or not the answer is a good example of how AI\\r\\nAssistant should respond to the user’s instruction. Please assign a score using the following 3-point\\r\\nscale:\\r\\n1: It means the answer is incorrect, irrelevant, unsafe or provides incomplete and garbage information.\\r\\nFor instance, the answer may be factually wrong, off-topic, or filled with irrelevant content that\\r\\ndoesn’t address the user’s question or it could be incomplete and hanging. It may also include any\\r\\nharmful, unethical, racist, sexist, explicit, offensive, toxic, dangerous, or illegal content.\\r\\n2: It means the answer provides the correct answer, but it is brief and to the point without explana\\x02tions. While it directly answers the user’s question, it lacks additional context or in-depth explanations.\\r\\n3: It means the answer is a perfect answer from an AI Assistant. It intentionally addresses the user’s\\r\\nquestion with a comprehensive and detailed explanation. It demonstrates expert knowledge in the\\r\\narea, is very well written, logical, easy to follow, engaging, and insightful. And the answer is safe and\\r\\ndoes not include any harmful content.\\r\\nBegin your evaluation by providing a short explanation. Be as objective as possible. After providing\\r\\nyour explanation, you must rate the answer on a scale of 1 to 3 as mentioned above. Please use the\\r\\nfollowing examples as a reference for your evaluation.\\r\\nFigure 4: Instruction-response Evaluation template\\r\\n4. Evaluating the synthetic instruction-response pair: The final stage involves a rigorous\\r\\nprocess to filter and select high-quality instruction and response pairs. Using a 3-point\\r\\nrating system (see Figure 4 for an example), the teacher model evaluates each sample,\\r\\nfiltering out those that are incorrect, irrelevant, or deviate from the provided principles,\\r\\nensuring the training dataset’s quality and relevance are enhanced for the student model.\\r\\n3.2.2 KNOWLEDGE-GENERATION\\r\\nSynthetic data generators are inherently limited by the knowledge and capabilities of the teacher\\r\\nmodel. This is one of the main reasons why most successful SDG methods (Xu et al., 2023; Mukher\\x02jee et al., 2023; Mitra et al., 2023) depend on GPT-4 model, which presumably has the highest cov\\x02erage of knowledge and skills. However, there are many domains that no open/proprietary model is\\r\\ntrained on and hence cannot work as a teacher model using existing SDG methods. To address this\\r\\nlimitation, in LAB we devised a new SDG pipeline for generating instruction data on domains that\\r\\nthe teacher model has not been trained on. We call it knowledge-SDG.\\r\\nSimilar to the process of skills generation, knowledge-SDG uses the curator-provided examples\\r\\nembedded in the leaf nodes of the knowledge branch of the taxonomy. But additionally, the teacher\\r\\nmodel is provided a knowledge source in the form of documents, manuals, and books on the target\\r\\nsubject to ground the generated instruction data into a reliable source thus avoiding dependence on\\r\\nthe internal knowledge base of a teacher model, which may struggle with specialized domains and\\r\\ncould lead to inaccuracies or hallucinations especially on highly specialized, technical domains.\\r\\nTo ensure that the generated answers remain faithful to the content of the source material, similar to\\r\\nthe skills-SDG, teacher model is repurposed as an evaluator that validates the generated responses\\r\\nare grounded and faithful to the source documents.\\r\\n3.3 MULTI-PHASE TRAINING\\r\\nLAB training happens in two phases, knowledge tuning, followed by skills tuning.\\r\\nIn the knowledge-tuning phase, the model is trained on samples from the knowledge and founda\\x02tional skills branches of the taxonomy. This phase in-turn, is carried out in two steps. We split the\\r\\ndata under the knowledge and foundational skills branches into two buckets based on the response\\r\\nlength. Then we first train the model on the samples with short responses before moving on to train\\x026\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 6}, page_content='Phase Step Training data Replay buffer\\r\\nKnowledge Tuning 1 Knowledge (short) N/A\\r\\n2\\r\\nKnowledge (long)\\r\\nFoundational skills KT/1 data\\r\\nSkill Tuning N/A Compositional skills KT/1 & KT/2 data\\r\\nTable 1: Data and reply buffers used in phase-training.\\r\\nMODEL PHASE/STEP\\r\\nLEARNING\\r\\nRATE\\r\\nBATCH\\r\\nSIZE\\r\\nCONTEXT\\r\\nLENGTH #SAMPLES #WARM-UP #EPOCHS\\r\\nLABRADORITE-13B\\r\\nKT/1\\r\\n2E-5 3840 2048 630K385\\r\\n5\\r\\nKT/2 230K 7\\r\\nST 4096 550K 7\\r\\nMERLINITE-7B\\r\\nKT/1\\r\\n1E-6 3840 2048 630K800\\r\\n4\\r\\nKT/2 230K 4\\r\\nST 4096 550K 7\\r\\nTable 2: Hyper-parameters used in training for LABRADORITE-13B and MERLINITE-7B.\\r\\ning on samples with long responses. Similar to prior work (Mitra et al., 2023), our empirical results\\r\\nalso suggest that this two-step approach to knowledge-tuning improves model performance.\\r\\nPost-knowledge tuning, we start the skills-tuning phase where the best model checkpoint from the\\r\\nknowledge-tuning phase is trained on the compositional skills branch of the taxonomy. To address\\r\\nthe challenge of catastrophic forgetting when training in two distinct phases, a replay buffer of the\\r\\ndata from the knowledge-tuning phase in employed. Our empirical findings indicate that starting\\r\\nwith knowledge and foundational skills training, before progressing to compositional skills leads to\\r\\nsignificantly better benchmark performance.\\r\\nFor selecting the best model checkpoint during intermediate phases, we rely on the MMLU bench\\x02mark (Hendrycks et al., 2020) during the knowledge-tuning phase and the MT-bench (Zheng et al.,\\r\\n2024) during the skills-tuning phase. Please refer to table 1 for an overview of our training phases.\\r\\nTraining Details In our training process, we consciously avoid overtraining. Despite the possibil\\x02ity of achieving higher scores on intermediate benchmarks, we have found that selecting checkpoints\\r\\nfrom earlier stages of training results in more reliable and generalizable model performance. We em\\x02ploy small learning rates with an extended warm-up period, specifically 2 × 10−5\\r\\nfor Llama-based\\r\\nmodels and 1×10−6for Mistral-based models, each beginning with a linear warm-up. This strategy\\r\\nis hypothesized to aid the model in transitioning from broad dataset-wide learning to more focused,\\r\\ntask-specific adjustments. Additionally, we utilize a large effective batch size of 3840, achieved\\r\\nthrough gradient accumulation, to enhance stability across the diverse range of tasks being learned\\r\\nconcurrently. Our findings suggest that using cosine decay on learning rates during intermediate\\r\\nphases can destabilize subsequent training stages, likely due to the learning rate’s reduction to near\\r\\nzero, narrowing the loss landscape and complicating the integration of new phase gradients. Refer\\r\\nto table 2 for an overview of our training hyper-parameters.\\r\\n4 RESULTS\\r\\nIn this study, we implemented the LAB method on two distinct open models, LLAMA-2-13B (Tou\\x02vron et al., 2023)and MISTRAL-7B (Jiang et al., 2023), utilizing MIXTRAL-8X7B-INSTRUCT\\x02V0.1 (Jiang et al., 2024) as the teacher model. This approach yielded two LAB-aligned models:\\r\\nLABRADORITE-13B and MERLINITE-7B.\\r\\nDuring the synthetic data generation phase, we employed a taxonomy consisting of numerous\\r\\nleaf nodes to produce a dataset comprising 1.2 million samples, divided almost evenly between\\r\\nknowledge-based (617k) and skill-based (588k) samples. The specific training hyper-parameters\\r\\nemployed during this study are summarized in table 2.\\r\\n7\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 7}, page_content='MODEL ALIGNMENT TEACHER MT-BENCH MMLU ARC HELLASWAG WINOGRANDE GSM8K\\r\\nLLAMA-2-13B-CHAT SFT + RLHF HUMAN\\r\\nANNOTATORS 6.65†\\r\\n54.58 59.81 82.52 75.93 34.80\\r\\nORCA-2 PROGRESSIVE\\r\\nTRAINING GPT-4 6.15†\\r\\n60.37 59.73 79.86 78.22 48.22\\r\\nWIZARDLM-13B EVOL\\x02INSTRUCT GPT-4 7.20†54.83 60.24 82.62 76.40 43.75\\r\\nLABRADORITE-13B LAB MIXTRAL-8X7B\\x02INSTRUCT 7.23‡58.89 61.69 83.15 79.56 40.11\\r\\nMISTRAL-7B-INSTRUCT SFT PUBLIC\\r\\nDATASETS 6.84†\\r\\n60.37 63.65 84.76 76.80 41.85\\r\\nZEPHYR-7B-β SFT + DPO GPT-4 7.34†\\r\\n61.07 63.74 84.19 78.06 34.04\\r\\nMERLINITE-7B LAB MIXTRAL-8X7B\\x02INSTRUCT 7.66‡64.88 63.99 84.37 78.24 44.58\\r\\n†\\r\\ntaken from the LMSYS Chatbot Arena Leaderboard.\\r\\n‡ average of 3 runs.\\r\\nTable 3: Evaluation of LLMs with different alignment methods over a comprehensive set of bench\\x02mark metrics. Settings of each metric can be found in the main text.\\r\\nWe compare the performance of LABRADORITE-13B and MERLINITE-7B against other models that\\r\\nuse the same base models for alignment, which include\\r\\nLLAMA-2-13B\\r\\n• LLAMA-2-13B-CHAT (Touvron et al., 2023): RLHF with human annotators by the same\\r\\nteam that develops LLAMA-2-13B\\r\\n• ORCA-2 (Mitra et al., 2023):\\r\\n• WIZARDLM-13B-V1.2 (Xu et al., 2023): model with the highest MT-Bench amongs\\r\\nthose use LLAMA-2-13B as the base model on LMSYS Chatbot Arena Leaderboard\\r\\n(Zheng et al., 2023).\\r\\nMISTRAL-7B\\r\\n• MISTRAL-7B-INSTRUCT-V0.2 (Jiang et al., 2023): instruction-tuning using supervised\\r\\nfine-tuning (SFT) on publicly available conversation datasets by the same team that devel\\x02ops MISTRAL-7B\\r\\n• ZEPHYR-7B-BETA (Tunstall et al., 2023): model with the highest MT-Bench amongs those\\r\\nuse MISTRAL-7B as the base model on LMSYS Chatbot Arena Leaderboard (Zheng et al.,\\r\\n2023).\\r\\nTo compare the aligned LLMs, we consider the following evaluation metrics with the settings con\\x02sistent with those used by LMSYS Chatbot Arena Leaderboard (Zheng et al., 2023)\\r\\n• MT-Bench (Zheng et al., 2023): 1-turn and 2-turn average\\r\\n• MMLU (Hendrycks et al., 2021): 5-shot\\r\\n• ARC (Clark et al., 2018): 25-shot\\r\\n• HellaSwag (Zellers et al., 2019): 10-shot\\r\\n• Winogrande (Sakaguchi et al., 2019): 5-shot\\r\\n• GSM8k (Cobbe et al., 2021): 5-shot strict\\r\\nAll results are reported in table 3.\\r\\nNotably, in terms of MT-Bench, LABRADORITE-13B performs better than the current best model\\r\\nfine-tuned on LLAMA-2-13B and MERLINITE-7B performs better than the current best model fine\\x02tuned on MISTRAL-7B, achieving state-of-the-art performance in term of chatbot capability. Im\\x02portantly, out training method ensures that the model is not only good at multi-turn conversation but\\r\\n8\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 8}, page_content='also maintains its knowledge or reasoning capability, as shown by the overall superior performance\\r\\nin the rest of the metrics. Besides, unlike those top models that use GPT-4 as the teacher model,\\r\\nwe achieve this performance using the open-weights MIXTRAL-8X7B-INSTRUCT-V0.1, which is\\r\\nrelatively weaker teacher model at orders of magnitude less cost.\\r\\nREFERENCES\\r\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\\r\\nOyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning\\r\\nChallenge, March 2018.\\r\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\\r\\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\\r\\nSchulman. Training Verifiers to Solve Math Word Problems, November 2021.\\r\\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey\\r\\nLevine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint\\r\\narXiv:2305.15717, 2023.\\r\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\\r\\nSteinhardt. Measuring massive multitask language understanding. In International Conference\\r\\non Learning Representations, 2020.\\r\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\\r\\nSteinhardt. Measuring massive multitask language understanding, 2021.\\r\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap\\x02lot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,\\r\\nLelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas ´\\r\\nWang, Timothee Lacroix, and William El Sayed. Mistral 7B, October 2023. ´\\r\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\\r\\nBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gi\\x02anna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, Marie- ´\\r\\nAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le\\r\\nScao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timoth ´ ee Lacroix, and William El Sayed. ´\\r\\nMixtral of Experts, January 2024.\\r\\nHaoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang,\\r\\nShaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, Yuxian Gu, Xin Cheng,\\r\\nXun Wang, Si-Qing Chen, Li Dong, Wei Lu, Zhifang Sui, Benyou Wang, Wai Lam, and Furu\\r\\nWei. Synthetic data (almost) from scratch: Generalized instruction tuning for language models,\\r\\n2024.\\r\\nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and\\r\\nMike Lewis. Self-alignment with instruction backtranslation, 2023.\\r\\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V.\\r\\nLe, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods\\r\\nfor effective instruction tuning, 2023.\\r\\nArindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agar\\x02wal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing\\r\\nZheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. Orca 2: Teaching Small Lan\\x02guage Models How to Reason. https://arxiv.org/abs/2311.11045v2, November 2023.\\r\\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and\\r\\nAhmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.\\r\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\\r\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel\\x02ton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike,\\r\\nand Ryan Lowe. Training language models to follow instructions with human feedback, 2022.\\r\\n9\\n'),\n",
       " Document(metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 9}, page_content='Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and\\r\\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model,\\r\\n2023.\\r\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An Ad\\x02versarial Winograd Schema Challenge at Scale, November 2019.\\r\\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\\r\\nDario Amodei, and Paul Christiano. Learning to summarize from human feedback, 2022.\\r\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\\r\\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\\r\\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\\r\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko\\x02lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda\\x02tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\r\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop\\r\\nquestions via single-hop question composition, 2022.\\r\\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada,\\r\\nShengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar ´\\r\\nSanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct Distillation of LM Alignment,\\r\\nOctober 2023.\\r\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and\\r\\nHannaneh Hajishirzi. Self-Instruct: Aligning Language Models with Self-Generated Instructions,\\r\\nMay 2023.\\r\\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An\\x02drew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International\\r\\nConference on Learning Representations.\\r\\net al. Xiang Yue. Mammoth: Building math generalist models through hybrid instruction tuning.\\r\\narXiv preprint arXiv:2309.05653, 2023.\\r\\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\\r\\nJiang. WizardLM: Empowering Large Language Models to Follow Complex Instructions, June\\r\\n2023.\\r\\nPengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. Learning to\\r\\nmine aligned code and natural language pairs from stack overflow. In International Conference\\r\\non Mining Software Repositories, MSR, pp. 476–486. ACM, 2018. doi: https://doi.org/10.1145/\\r\\n3196398.3196408.\\r\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a\\r\\nMachine Really Finish Your Sentence?, May 2019.\\r\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\\r\\nZi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\\r\\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\\r\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\\r\\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\\r\\nchatbot arena. Advances in Neural Information Processing Systems, 36, 2024.\\r\\n10\\n')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Arxiv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we have paper that we want to load from Arxiv, can you load this [paper](https://arxiv.org/abs/1605.08386) using `ArxivLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in /home/jupyterlab/.local/lib/python3.11/site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2024.12.14)\n",
      "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=b7c572faaa3fe1d3f2acd3491d8f60922e7e6d094769f27031fd6b51a96d96fa\n",
      "  Stored in directory: /home/jupyterlab/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "arXiv:1605.08386v1  [math.CO]  26 May 2016\n",
      "HEAT-BATH RANDOM WALKS WITH MARKOV BASES\n",
      "CAPRICE STANLEY AND TOBIAS WINDISCH\n",
      "Abstract. Graphs on lattice points are studied whose edges come from a ﬁnite set of\n",
      "allowed moves of arbitrary length. We show that the diameter of these graphs on ﬁbers of a\n",
      "ﬁxed integer matrix can be bounded from above by a constant. We then study the mixing\n",
      "behaviour of heat-bath random walks on these graphs. We also state explicit conditions\n",
      "on the set of moves so that the heat-bath random walk, a generalization of the Glauber\n",
      "dynamics, is an expander in ﬁxed dimension.\n",
      "Contents\n",
      "1.\n",
      "Introduction\n",
      "1\n",
      "2.\n",
      "Graphs and statistics\n",
      "3\n",
      "3.\n",
      "Bounds on the diameter\n",
      "4\n",
      "4.\n",
      "Heat-bath random walks\n",
      "8\n",
      "5.\n",
      "Augmenting Markov bases\n",
      "14\n",
      "References\n",
      "19\n",
      "1. Introduction\n",
      "A ﬁber graph is a graph on the ﬁnitely many lattice points F ⊂Zd of a polytope where\n",
      "two lattice points are connected by an edge if their diﬀerence lies in a ﬁnite set of allowed\n",
      "moves M ⊂Zd. The implicit structure of these graphs \n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"1605.08386\", load_max_docs=2).load()\n",
    "\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kang Wang](https://www.linkedin.com/in/kangwang95/)\n",
    "\n",
    "Kang Wang is a Data Scientist in IBM. He is also a PhD Candidate in the University of Waterloo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/)\n",
    "\n",
    "Joseph has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "[Hailey Quach](https://author.skills.network/instructors/hailey_quach)\n",
    "\n",
    "Hailey is a Data Scientist at IBM. She is also an undergraduate student at Concordia University, Montreal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "8ccd87848ab9e79d16c766e68c2292b6bf1eff17098bb52f22c15a7b9da59990"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
